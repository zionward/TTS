<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Tour of Time Series Analysis with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A Tour of Time Series Analysis with R">
  <meta name="generator" content="bookdown 0.1.5 and GitBook 2.6.7">

  <meta property="og:title" content="A Tour of Time Series Analysis with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tts.smac-group.com/" />
  
  
  <meta name="github-repo" content="SMAC-Group/TTS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Tour of Time Series Analysis with R" />
  
  
  

<meta name="author" content="James Balamuta, Stéphane Guerrier and Roberto Molinari">

<meta name="date" content="2016-08-19">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="autocorrelation-and-stationarity.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { extensions: ["AMSmath.js"], 
         equationNumbers: { autoNumber: "AMS" },
         Macros: {
          notimplies: "\\nRightarrow",
          real: "\\mathbb{R}",
          integers: "\\mathbb{Z}",
          natural: "\\mathbb{N}",
          rational: "\\mathbb{Q}",
          irrational: "\\mathbb{P}",
          0: "\\boldsymbol{0}",
          I: "\\boldsymbol{\\mathbf{I}}",
          S: "\\boldsymbol{S}",
          y: "\\boldsymbol{y}",
          X: "\\boldsymbol{X}",
          C: "\\text{C}",
          btheta: "\\boldsymbol{\\theta}",
          epsilon: "\\varepsilon",
          bbeta: "\\boldsymbol{\\beta}", 
          bepsilon: "\\boldsymbol{\\varepsilon}", 
          norm: "\\mathcal{N}",
          KL: "\\text{KL}",
          AIC: "\\text{AIC}", 
          BIC: "\\text{BIC}", 
          mean: ["\\operatorname{mean}"],
          var: ["\\operatorname{var}"],
          tr: ["\\operatorname{tr}"],
          cov: ["\\operatorname{cov}"],
          corr: ["\\operatorname{corr}"],
          argmax: ["\\operatorname{argmax}"],
          argmin: ["\\operatorname{argmin}"],
          card: ["\\operatorname{card}"],
          diag: ["\\operatorname{diag}"],
          rank: ["\\operatorname{rank}"],
          length: ["\\operatorname{length}"]
    }
  }
});
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="styling/style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tour of Time Series Analysis with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bibliographic-note"><i class="fa fa-check"></i>Bibliographic Note</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#rendering-mathematical-formulae"><i class="fa fa-check"></i>Rendering Mathematical Formulae</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#r-code-conventions"><i class="fa fa-check"></i>R Code Conventions</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#time-series"><i class="fa fa-check"></i><b>1.1</b> Time Series</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#exploratory-data-analysis-for-time-series"><i class="fa fa-check"></i><b>1.2</b> Exploratory Data Analysis for Time Series</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#basic-time-series-models"><i class="fa fa-check"></i><b>1.3</b> Basic Time Series Models</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#white-noise-processes"><i class="fa fa-check"></i><b>1.3.1</b> White noise processes</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#random-walk-processes"><i class="fa fa-check"></i><b>1.3.2</b> Random Walk Processes</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#autoregressive-process-of-order-1"><i class="fa fa-check"></i><b>1.3.3</b> Autoregressive Process of Order 1</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#moving-average-process-of-order-1"><i class="fa fa-check"></i><b>1.3.4</b> Moving Average Process of Order 1</a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#linear-drift"><i class="fa fa-check"></i><b>1.3.5</b> Linear Drift</a></li>
<li class="chapter" data-level="1.3.6" data-path="introduction.html"><a href="introduction.html#composite-stochastic-processes"><i class="fa fa-check"></i><b>1.3.6</b> Composite Stochastic Processes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html"><i class="fa fa-check"></i><b>2</b> Autocorrelation and Stationarity</a><ul>
<li class="chapter" data-level="2.1" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#the-autocorrelation-and-autocovariance-functions"><i class="fa fa-check"></i><b>2.1</b> The Autocorrelation and Autocovariance Functions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#definitions"><i class="fa fa-check"></i><b>2.1.1</b> Definitions</a></li>
<li class="chapter" data-level="2.1.2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#a-fundamental-representation"><i class="fa fa-check"></i><b>2.1.2</b> A Fundamental Representation</a></li>
<li class="chapter" data-level="2.1.3" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#admissible-autocorrelation-functions"><i class="fa fa-check"></i><b>2.1.3</b> Admissible autocorrelation functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#stationarity"><i class="fa fa-check"></i><b>2.2</b> Stationarity</a><ul>
<li class="chapter" data-level="2.2.1" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#definitions-1"><i class="fa fa-check"></i><b>2.2.1</b> Definitions</a></li>
<li class="chapter" data-level="2.2.2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#assessing-weak-stationarity-of-time-series-models"><i class="fa fa-check"></i><b>2.2.2</b> Assessing Weak Stationarity of Time Series Models</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#estimation-of-the-mean-function"><i class="fa fa-check"></i><b>2.3</b> Estimation of the Mean Function</a></li>
<li class="chapter" data-level="2.4" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>2.4</b> Sample Autocovariance and Autocorrelation Functions</a></li>
<li class="chapter" data-level="2.5" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#robustness-issues"><i class="fa fa-check"></i><b>2.5</b> Robustness Issues</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic-models.html"><a href="basic-models.html"><i class="fa fa-check"></i><b>3</b> Basic Models</a><ul>
<li class="chapter" data-level="3.1" data-path="basic-models.html"><a href="basic-models.html#the-backshift-operator"><i class="fa fa-check"></i><b>3.1</b> The Backshift Operator</a></li>
<li class="chapter" data-level="3.2" data-path="basic-models.html"><a href="basic-models.html#white-noise"><i class="fa fa-check"></i><b>3.2</b> White Noise</a></li>
<li class="chapter" data-level="3.3" data-path="basic-models.html"><a href="basic-models.html#moving-average-process-of-order-q-1-a.k.a-ma1"><i class="fa fa-check"></i><b>3.3</b> Moving Average Process of Order q = 1 a.k.a MA(1)</a></li>
<li class="chapter" data-level="3.4" data-path="basic-models.html"><a href="basic-models.html#drift"><i class="fa fa-check"></i><b>3.4</b> Drift</a></li>
<li class="chapter" data-level="3.5" data-path="basic-models.html"><a href="basic-models.html#random-walk"><i class="fa fa-check"></i><b>3.5</b> Random Walk</a></li>
<li class="chapter" data-level="3.6" data-path="basic-models.html"><a href="basic-models.html#random-walk-with-drift"><i class="fa fa-check"></i><b>3.6</b> Random Walk with Drift</a></li>
<li class="chapter" data-level="3.7" data-path="basic-models.html"><a href="basic-models.html#autoregressive-process-of-order-p-1-a.k.a-ar1"><i class="fa fa-check"></i><b>3.7</b> Autoregressive Process of Order p = 1 a.k.a AR(1)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="arma.html"><a href="arma.html"><i class="fa fa-check"></i><b>4</b> ARMA</a><ul>
<li class="chapter" data-level="4.1" data-path="arma.html"><a href="arma.html#definition"><i class="fa fa-check"></i><b>4.1</b> Definition</a></li>
<li class="chapter" data-level="4.2" data-path="arma.html"><a href="arma.html#ma-ar-operators"><i class="fa fa-check"></i><b>4.2</b> MA / AR Operators</a></li>
<li class="chapter" data-level="4.3" data-path="arma.html"><a href="arma.html#redundancy"><i class="fa fa-check"></i><b>4.3</b> Redundancy</a></li>
<li class="chapter" data-level="4.4" data-path="arma.html"><a href="arma.html#causal-invertible"><i class="fa fa-check"></i><b>4.4</b> Causal + Invertible</a></li>
<li class="chapter" data-level="4.5" data-path="arma.html"><a href="arma.html#estimation-of-parameters"><i class="fa fa-check"></i><b>4.5</b> Estimation of Parameters</a><ul>
<li class="chapter" data-level="4.5.1" data-path="arma.html"><a href="arma.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>4.5.1</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="4.5.2" data-path="arma.html"><a href="arma.html#mle-for-sigma-2-on-ar1-with-mean-mu"><i class="fa fa-check"></i><b>4.5.2</b> MLE for <span class="math inline">\(\sigma ^2\)</span> on <span class="math inline">\(AR(1)\)</span> with mean <span class="math inline">\(\mu\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="arma.html"><a href="arma.html#method-of-moments"><i class="fa fa-check"></i><b>4.6</b> Method of Moments</a><ul>
<li class="chapter" data-level="4.6.1" data-path="arma.html"><a href="arma.html#method-of-moments---arp"><i class="fa fa-check"></i><b>4.6.1</b> Method of Moments - AR(p)</a></li>
<li class="chapter" data-level="4.6.2" data-path="arma.html"><a href="arma.html#yule-walker"><i class="fa fa-check"></i><b>4.6.2</b> Yule-Walker</a></li>
<li class="chapter" data-level="4.6.3" data-path="arma.html"><a href="arma.html#estimates"><i class="fa fa-check"></i><b>4.6.3</b> Estimates</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="arma.html"><a href="arma.html#prediction-forecast"><i class="fa fa-check"></i><b>4.7</b> Prediction (Forecast)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-regression-with-autocorrelated-errors.html"><a href="linear-regression-with-autocorrelated-errors.html"><i class="fa fa-check"></i><b>5</b> Linear Regression with Autocorrelated Errors</a></li>
<li class="chapter" data-level="6" data-path="state-space-models.html"><a href="state-space-models.html"><i class="fa fa-check"></i><b>6</b> State-Space Models</a></li>
<li class="chapter" data-level="7" data-path="time-series-models-of-heteroskedasticity.html"><a href="time-series-models-of-heteroskedasticity.html"><i class="fa fa-check"></i><b>7</b> Time Series Models of Heteroskedasticity</a></li>
<li class="divider"></li>
<li><a href="https://github.com/SMAC-Group/TTS" target="blank">&copy; 2016 Balamuta, Guerrier, Molinari</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tour of Time Series Analysis with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<!--
> "One damned thing after another." ~ R. A. Fisher
>
-->
<blockquote>
<p><em>Prévoir consiste à projeter dans l’avenir ce qu’on a perçu dans le passé.</em> Henri Bergson</p>
</blockquote>
<!--
>
> *Hâtons-nous; le temps fuit, et nous entraîne avec soi : le moment où je parle est déjà loin de moi*. Nicolas Boileau
>
-->
<p>After reading this chapter you will be able to:</p>
<ul>
<li>Describe what a <em>time series</em> is.</li>
<li>Perform explore data analysis on time series data.</li>
<li>Evaluate different characteristics of a time series.</li>
<li>Classify basic time series models by equation and plots.</li>
<li>Manipulate a time series equation using <em>backsubstitution</em>.</li>
</ul>
<div id="time-series" class="section level2">
<h2><span class="header-section-number">1.1</span> Time Series</h2>
<p>Generally speaking a <em>time series</em> (or stochastic process) corresponds to set of “repeated” observations of the same variable such as price of a financial asset or temperature in a given location. In terms of notation a time series is often written as</p>
<p><span class="math display">\[\left(X_1, X_2, ..., X_n \right) \;\;\; \text{ or } \;\;\; \left(X_t\right)_{t = 1,...,n}.\]</span></p>
<p>The time index <span class="math inline">\(t\)</span> is contained within either the set of reals, <span class="math inline">\(\real\)</span>, or integers, <span class="math inline">\(\integers\)</span>. When <span class="math inline">\(t \in \real\)</span>, the time series becomes a <em>continuous-time</em> stochastic process such a Brownian motion, a model used to represent the random movement of particles within a suspended liquid or gas, or an ElectroCardioGram (ECG) signal, which corresponds to the pulpitations of the heart. <!-- as shown in Figure \@ref(fig:ecg-img).--> However, within this text, we will limit ourselves to the case the later case. That is, the focus will be on cases where <span class="math inline">\(t \in \integers\)</span> better known as <em>discrete-time</em> processes. <em>Discrete-time</em> processes are where a variable is measured sequentially at fixed and equally spaced intervals in time akin to <a href="introduction.html#fig:numberline">1.1</a>. This implies that we will assume two tenents:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(t\)</span> is not random e.g. the time at which each observation is measured is known, and</li>
<li>the time between two consequtive observation is constant.</li>
</ol>
<!-- JAMES: I commented this figure, not sure it is very useful. If you want to keep it let me know...
<div class="figure" style="text-align: center">
<img src="images/ecg.png" alt="Continuous-time view that is used by an ECG"  />
<p class="caption">(\#fig:ecg-img)Continuous-time view that is used by an ECG</p>
</div>
-->
<div class="figure" style="text-align: center"><span id="fig:numberline"></span>
<img src="tts_files/figure-html/numberline-1.png" alt="Discrete-time can be thought of as viewing a number line with equally spaced points." width="672" />
<p class="caption">
Figure 1.1: Discrete-time can be thought of as viewing a number line with equally spaced points.
</p>
</div>
<p>Moreover, the term “time series” can also represent a probability model for set of observations. For example, one of the fundamental probability models used in time series analysis is called a <em>white noise</em> process and is defined as</p>
<p><span class="math display">\[W_t \mathop \sim \limits^{iid} N(0, \sigma^2).\]</span></p>
<p>This statement simply means that <span class="math inline">\((W_t)\)</span> is normally distributed and independent over time. This model may appear to be dull but as we will see it is a crucial component to constructing intricate and riveting models. Unlike the white noise process, time series are typically <em>not</em> independent over time. Suppose that the temperature in Champaign is unusually low, then it is reasonable to assume that tomorrow’s temperature will also be low. Indeed, such behavior would suggest the existence of a dependency over time. The time series methods we will discuss in this text consists of parametric models used to characterize (or at least approximate) the joint distribution of <span class="math inline">\((X_t)\)</span>. Often, time series models can be decomposed into two components the first of which is what we call a <em>signal</em>, say <span class="math inline">\((Y_t)\)</span>, and the second component is a <em>noise</em>, say <span class="math inline">\((W_t)\)</span>, leading to the model</p>
<p><span class="math display">\[X_t = Y_t + W_t.\]</span></p>
<p>Typically, we have <span class="math inline">\(E[Y_t] \neq 0\)</span> while <span class="math inline">\(E[W_t] = 0\)</span> (although we may have <span class="math inline">\(E[W_t | W_{t-1}, ..., W_1] \neq 0\)</span>). Such models impose some parametric structure which represents a convenient and flexible way of studying time series as well as a means to evalute <em>future</em> values of the series through forecasting. As we will see, predicting future values is one of the main aspects of time series analysis. However, making predictions is often a daunting task or as famously stated by Nils Bohr:</p>
<blockquote>
<p>“<em>Prediction is very difficult, especially about the future.</em>”</p>
</blockquote>
<p>There are plenty of examples predictions which were revealed to be completely erroneous. For example, Irving Fisher, Professor of Economics at Yale University, famously predicted three days before the 1929 crash:</p>
<blockquote>
<p>“<em>Stock prices have reached what looks like a permanently high plateau</em>”.</p>
</blockquote>
<p>Another example is Thomas Watson, president of IBM, who said in 1943:</p>
<blockquote>
<p>“<em>I think there is a world market for maybe five computers.</em>”</p>
</blockquote>
</div>
<div id="exploratory-data-analysis-for-time-series" class="section level2">
<h2><span class="header-section-number">1.2</span> Exploratory Data Analysis for Time Series</h2>
<p>When dealing with relatively small time series (e.g. a few thousands), it is often useful to look at a graph of the original data. Such graphs can be informative to “detect” some features of a time series such as trends and the presence of outliers.</p>
<p>Indeed, a trend is typically deemed present in a time series when the data exhibit some form of long term increase or decrease or combination of increases or decreases. Such trends could be linear or non-linear and represent a important part of the “signal” of a model. Here are few examples of non-linear trends:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Seasonal trends</strong> (periodic): These are the cyclical patterns which repeat after a fixed/regular time period. This could be due to business cycles (e.g. bust/recession, recovery).</p></li>
<li><p><strong>Non-seasonal trends</strong> (periodic): These patterns cannot be associated to seasonal variation and can for example to external variable. For example, impact of economic indicators on stock returns. Note that such trends are often hard to detect based on a graphical analysis of the data.</p></li>
<li><p><strong>“Other” trends</strong>: These trends have typically no regular patterns and are over a segment of time, known as a “window”, that change the statistical properties of a time series. A common example of such trends corresponds to vibrations observed before, during and after an earthquake.</p></li>
</ol>
<p><strong>Example:</strong> A traditional example of a time series is the quarterly earnings of the company Johnson and Johson. In the figure below, we present these earnings between 1960 and 1980:</p>
<p><img src="tts_files/figure-html/example_jj-1.png" width="672" /></p>
<p>One trait that the graph makes evident is the data contains a non-linear increasing trend as well as a yearly seasonal component. In addition, one can note that the <em>variability</em> of the data seems to increase with time. Being able to make such observations provides actionable information to select a suitable models for the data.</p>
<p>Moreover, when observing “raw” time series data it is also interesting to evaluate if some the following phenomenon occur:</p>
<ol style="list-style-type: decimal">
<li><strong>Change in Means:</strong> Does the mean of the process shift over time?</li>
<li><strong>Change in Variance:</strong> Does the variance of the process evolves with time?</li>
<li><strong>Change in State:</strong> Does the time series appear to change between “states” having distinct statistical properties?</li>
<li><strong>Outliers</strong> Does the time series contain some “extreme” observations? Note that this is typically difficult to assess visually.</li>
</ol>
<p><strong>Example:</strong> In the figure below, we present an example of displacement recorded during an earthquake as well as explosion.</p>
<p><img src="tts_files/figure-html/example_eq-1.png" width="672" /></p>
<p>From the graph, it can be observed that the statistical properties of the time series appear to change over time. For instance, the variance of the time series shifts at around <span class="math inline">\(t = 1150\)</span> for both series. The shift in variance also opens “windows” where there appears to be distinct states. In the case of the explosion data this is particularly relevant around <span class="math inline">\(t = 50, \cdots, 250\)</span> and then again from <span class="math inline">\(t = 1200, \cdots, 1500\)</span>. Even within these windows, there are “spikes” that could be considered as outliers most notably around <span class="math inline">\(t = 1200\)</span> for explosion series.</p>
<p>Next, we consider an example about high-frequency finance to illustate to limitation our current framework.</p>
<p><strong>Example:</strong> The figure below present the returns (i.e. informally speaking the changes in price) for Starbuck’s stock on the first of July 2011 during about 150 seconds (left panel) and about 400 minutes (right panel).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load packages</span>
<span class="kw">library</span>(timeDate)

<span class="co"># Load &quot;high-frequency&quot; Starbucks returns for Jul 01 2011</span>
<span class="kw">data</span>(sbux.xts, <span class="dt">package =</span> <span class="st">&quot;highfrequency&quot;</span>)

<span class="co"># Plot returns</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(sbux.xts[<span class="dv">1</span>:<span class="dv">89</span>], <span class="dt">main =</span> <span class="st">&quot; &quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Returns&quot;</span>)
<span class="kw">plot</span>(sbux.xts, <span class="dt">main =</span> <span class="st">&quot; &quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Returns&quot;</span>)</code></pre></div>
<p><img src="tts_files/figure-html/example_highfreq-1.png" width="672" /></p>
<p>It can be observed on the left panel that points are not equally spaced. Indeed, in high-frequency data interval between two points is typically not constant and is, even worse, a random variable. This implies that when a new observation will be available is in general unknown. On the right panel, one can observe that the variability of the data seems to change during the course of the trading day. Such phenomenon is well known in the finance community as a lot variation occurs at the start (and the end) of the day while the middle of the day is associated with small changes. Moreover, clear extreme observations can also be noted in this graph at around 09:30:34.</p>
<!-- Geatan: Could you please check and if you have the time expend with example a little -->
<p>Finally, ….</p>
<!--
Complete example on IMU to show the limit for such graph when n is too big.
-->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load packages</span>
<span class="kw">library</span>(gmwm)
<span class="kw">library</span>(imudata)

<span class="co"># Load IMU data</span>
<span class="kw">data</span>(imu6, <span class="dt">package =</span> <span class="st">&quot;imudata&quot;</span>)
Xt =<span class="st"> </span><span class="kw">gts</span>(imu6[,<span class="dv">1</span>], <span class="dt">name =</span> <span class="st">&quot;Gyroscope data&quot;</span>, <span class="dt">unit =</span> <span class="st">&quot;sec&quot;</span>, <span class="dt">freq =</span> <span class="dv">100</span>)

<span class="co"># Plot gryoscope data</span>
<span class="kw">autoplot</span>(Xt) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Error (rad/s^2)&quot;</span>)</code></pre></div>
<p><img src="tts_files/figure-html/example_IMU-1.png" width="672" /></p>
</div>
<div id="basic-time-series-models" class="section level2">
<h2><span class="header-section-number">1.3</span> Basic Time Series Models</h2>
<p>In this section, we introduce some simple time series models. Before doing so it is useful to define <span class="math inline">\(\Omega_t\)</span> as all the information avaiable up to time <span class="math inline">\(t-1\)</span>, i.e.</p>
<p><span class="math display">\[\Omega_t = \left(X_{t-1}, X_{t-2}, ..., X_0 \right).\]</span></p>
<p>As we will see this compact notation is quite useful.</p>
<div id="white-noise-processes" class="section level3">
<h3><span class="header-section-number">1.3.1</span> White noise processes</h3>
<p>The building block for most time series models is the Gaussian white noise process, which can be defined as</p>
<p><span class="math display">\[{W_t}\mathop \sim \limits^{iid} N\left( {0,\sigma _w^2} \right).\]</span></p>
<p>This definition implies that:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E[W_t | \Omega_t] = 0\)</span> for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\cov\left(W_t, W_{t-h} \right) = \boldsymbol{1}_{h = 0} \; \sigma^2\)</span> for all <span class="math inline">\(t, h\)</span>.</li>
</ol>
<p>Therefore, this process present an absence of temporal (or serial) dependence and is homoskedastic (i.e it has a constant variance). This definition can be generalized in two sorts of processes, the <em>weak</em> and <em>strong</em> white noise. The process <span class="math inline">\((W_t)\)</span> is a weak white noise if</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E[W_t] = 0\)</span> for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\var\left(W_t\right) = \sigma^2\)</span> for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\cov \left(W_t, W_{t-h}\right) = 0\)</span>, for all <span class="math inline">\(t\)</span>, and for all <span class="math inline">\(h \neq 0\)</span>.</li>
</ol>
<p>Note that this definition does not imply that <span class="math inline">\(W_t\)</span> and <span class="math inline">\(W_{t-h}\)</span> are independent (for <span class="math inline">\(h \neq 0\)</span>) but simply uncorrelated. However, the notion of indepence is used to define a <em>strong</em> white noise as</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E[W_t] = 0\)</span> and <span class="math inline">\(\var(W_t) = \sigma^2 &lt; \infty\)</span>, for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(F(W_t) = F(W_{t-h})\)</span>, for all <span class="math inline">\(t,h\)</span> (where <span class="math inline">\(F(W_t)\)</span> denotes the distribution of <span class="math inline">\(W_t\)</span>),</li>
<li><span class="math inline">\(W_t\)</span> and <span class="math inline">\(W_{t-h}\)</span> are independent for all <span class="math inline">\(t\)</span> and for all <span class="math inline">\(h \neq 0\)</span>.</li>
</ol>
<p>It is clear from these definitions that if a process is a strong white noise it is also a weak white noise. However, the converse is not true a shown in the following example:</p>
<p><strong>Example</strong>: Let <span class="math inline">\(X_t \mathop \sim \limits^{iid} F_t\)</span>, where <span class="math inline">\(F_t\)</span> denote a Student distribution with <span class="math inline">\(t\)</span> degrees of freedom. Such process is a weak but not a strong white noise.</p>
<p>The code below presents an example of how to simulate a Gaussian white noise process</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># This code simulate a gaussian white noise process</span>
n =<span class="st"> </span><span class="dv">100</span>                               <span class="co"># process length</span>
sigma2 =<span class="st"> </span><span class="dv">1</span>                            <span class="co"># process variance</span>
Xt =<span class="st"> </span><span class="kw">gen.gts</span>(<span class="kw">WN</span>(<span class="dt">sigma2 =</span> sigma2), <span class="dt">N =</span> n)
<span class="kw">plot</span>(Xt)</code></pre></div>
<p><img src="tts_files/figure-html/example_WN-1.png" width="672" /></p>
</div>
<div id="random-walk-processes" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Random Walk Processes</h3>
<p>The term <em>random walk</em> was first introduce by Karl Pearson in the early 19 hunders. As for the white noise, there exist a large range of random walk processes. For example, one of the simplest form of random walk are be explained as follows: suppose that you are walking on campus and your next step can either be on your left, your right, forward or backward (each with equal probability). Two realizations of such processes are represented below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Function computes direction random walk moves</span>
RW2dimension =<span class="st"> </span>function(<span class="dt">steps =</span> <span class="dv">100</span>){
  <span class="co"># Initial matrix</span>
  step_direction =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, steps<span class="dv">+1</span>, <span class="dv">2</span>)
  
  <span class="co"># Start random walk</span>
  for (i in <span class="kw">seq</span>(<span class="dv">2</span>, steps<span class="dv">+1</span>)){
    <span class="co"># Draw a random number from U(0,1)</span>
    rn =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
    
    <span class="co"># Go right if rn \in [0,0.25)</span>
    if (rn &lt;<span class="st"> </span><span class="fl">0.25</span>) {step_direction[i,<span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span>}
      
    <span class="co"># Go left if rn \in [0.25,0.5)</span>
    if (rn &gt;=<span class="st"> </span><span class="fl">0.25</span> &amp;&amp;<span class="st"> </span>rn &lt;<span class="st"> </span><span class="fl">0.5</span>) {step_direction[i,<span class="dv">1</span>] =<span class="st"> </span>-<span class="dv">1</span>}
    
    <span class="co"># Go forward if rn \in [0.5,0.75)</span>
    if (rn &gt;=<span class="st"> </span><span class="fl">0.5</span> &amp;&amp;<span class="st"> </span>rn &lt;<span class="st"> </span><span class="fl">0.75</span>) {step_direction[i,<span class="dv">2</span>] =<span class="st"> </span><span class="dv">1</span>}
    
    <span class="co"># Go backward if rn \in [0.75,1]</span>
    if (rn &gt;=<span class="st"> </span><span class="fl">0.75</span>) {step_direction[i,<span class="dv">2</span>] =<span class="st"> </span>-<span class="dv">1</span>}
  }
  
  <span class="co"># Cumulative steps</span>
  position =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">cumsum</span>(step_direction[, <span class="dv">1</span>]),
                        <span class="dt">y =</span> <span class="kw">cumsum</span>(step_direction[, <span class="dv">2</span>]))
  
  <span class="co"># Mark start and stop locations</span>
  start_stop =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, position[steps<span class="dv">+1</span>, <span class="dv">1</span>]),
                          <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, position[steps<span class="dv">+1</span>, <span class="dv">2</span>]),
                          <span class="dt">type =</span> <span class="kw">factor</span>(<span class="kw">c</span>(<span class="st">&quot;Start&quot;</span>,<span class="st">&quot;End&quot;</span>),
                                        <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;Start&quot;</span>,<span class="st">&quot;End&quot;</span>)))
  
  <span class="co"># Plot results</span>
  <span class="kw">ggplot</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> position) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">data =</span> start_stop, <span class="kw">aes</span>(<span class="dt">color =</span> type), <span class="dt">size =</span> <span class="dv">4</span>) +
<span class="st">    </span><span class="kw">theme_bw</span>() +<span class="st"> </span>
<span class="st">    </span><span class="kw">labs</span>(
      <span class="dt">x =</span> <span class="st">&quot;X-position&quot;</span>,
      <span class="dt">y =</span> <span class="st">&quot;Y-position&quot;</span>,
      <span class="dt">title =</span> <span class="kw">paste</span>(<span class="st">&quot;2D random walk with&quot;</span>, steps, <span class="st">&quot;steps&quot;</span>),
      <span class="dt">color =</span> <span class="st">&quot;&quot;</span>
    ) +<span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>))
}

<span class="co"># Plot 2D random walk with 10^2 and 10^5 steps</span>
<span class="kw">set.seed</span>(<span class="dv">2</span>)

<span class="kw">RW2dimension</span>(<span class="dt">steps =</span> <span class="dv">10</span>^<span class="dv">2</span>)
<span class="kw">RW2dimension</span>(<span class="dt">steps =</span> <span class="dv">10</span>^<span class="dv">5</span>) </code></pre></div>
<p><img src="tts_files/figure-html/RW2d-1.png" width="672" /><img src="tts_files/figure-html/RW2d-2.png" width="672" /></p>
<p>Such processes inspired Karl Pearson’s famous quote that</p>
<blockquote>
<p>“<em>the most likely place to find a drunken walker is somewhere near his starting point.</em>”</p>
</blockquote>
<p>Empirical evidence of this phenomenon is not too hard to find on a Friday night in Champaign. In this class, we only consider one very specific form of randon walk, namely the Gaussian random walk which can be defined as:</p>
<p><span class="math display">\[X_t = X_{t-1} + W_t,\]</span></p>
<p>where <span class="math inline">\(W_t\)</span> is a Gaussian white noise and with initial condition <span class="math inline">\(X_0 = c\)</span> (typically <span class="math inline">\(c = 0\)</span>). This process can be expressed differently by <em>backsubstitution</em> as follows:</p>
<p><span class="math display">\[\begin{aligned}
  {X_t} &amp;= {X_{t - 1}} + {W_t} \\
   &amp;= \left( {{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
   &amp;= \vdots \\
  {X_t} &amp;= \sum\limits_{i = 1}^t {{W_i}} + X_0 =  \sum\limits_{i = 1}^t {{W_i}} + c \\ 
\end{aligned} \]</span></p>
<p>The code below presents an example of how to simulate a such process</p>
</div>
<div id="autoregressive-process-of-order-1" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Autoregressive Process of Order 1</h3>
<p>An autoregressive process of order 1 or AR(1) is a generalization of both the white noise and random walk process which are both special case of an AR(1). A (Gaussian) AR(1) process can be defined as</p>
<p><span class="math display">\[{X_t} = {\phi}{X_{t - 1}} + {W_t},\]</span></p>
<p>where <span class="math inline">\(W_t\)</span> is a Gaussian white noise. Clearly, an AR(1) with <span class="math inline">\(\phi = 0\)</span> is a Gaussian white noise and when <span class="math inline">\(\phi = 1\)</span> the process becomes a random walk.</p>
<p><strong>Remark:</strong> We generally assume that an AR(1) (as well as other time series models) have zero mean. The reason for this assumption is only to simplfy the notation but it is easy to consider an AR(1) process around an arbitrary mean <span class="math inline">\(\mu\)</span>, i.e.</p>
<p><span class="math display">\[\left(X_t - \mu\right) = \phi \left(X_{t-1} - \mu \right) + W_t,\]</span></p>
<p>which is of course equivalent to</p>
<p><span class="math display">\[X_t = \left(1 - \phi \right) \mu + \phi X_{t-1} + W_t.\]</span></p>
<p>Thus, we will generally onyl work with zero mean processes since adding means is simple.</p>
<p><strong>Remark:</strong> An AR(1) is in fact a linear combination of the past realisations of the white noise <span class="math inline">\(W_t\)</span>. Indeed, we have</p>
<p><span class="math display">\[\begin{aligned}
 {X_t} &amp;= {\phi_t}{X_{t - 1}} + {W_t} 
   = {\phi}\left( {{\phi}{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
   &amp;= \phi^2{X_{t - 2}} + {\phi}{W_{t - 1}} + {W_t} 
   = {\phi^t}{X_0} + \sum\limits_{i = 0}^{t - 1} {\phi^i{W_{t - i}}}.
\end{aligned}\]</span></p>
<p>Under the assumption of infinite past (i.e. <span class="math inline">\(t \in \mathbb{Z}\)</span>) and <span class="math inline">\(|\phi| &lt; 1\)</span>, we obtain</p>
<p><span class="math display">\[X_t = \sum\limits_{i = 0}^{\infty} {\phi^i {W_{t - i}}},\]</span></p>
<p>since <span class="math inline">\(\operatorname{lim}_{i \to \infty} \; {\phi^i}{X_{t-i}} = 0\)</span>.</p>
<p>The code below presents an example of how an AR(1) can be simulated</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># This code simulate a gaussian random walk process</span>
n =<span class="st"> </span><span class="dv">100</span>                               <span class="co"># process length</span>
phi =<span class="st"> </span><span class="fl">0.5</span>                             <span class="co"># phi parameter</span>
sigma2 =<span class="st"> </span><span class="dv">1</span>                            <span class="co"># innovation variance</span>
Xt =<span class="st"> </span><span class="kw">gen.gts</span>(<span class="kw">AR1</span>(<span class="dt">phi =</span> phi, <span class="dt">sigma2 =</span> sigma2), <span class="dt">N =</span> n)
<span class="kw">plot</span>(Xt)</code></pre></div>
<p><img src="tts_files/figure-html/example_AR1-1.png" width="672" /></p>
</div>
<div id="moving-average-process-of-order-1" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Moving Average Process of Order 1</h3>
<p>As we have seen in the previous example, an AR(1) can be expressed as a linear combination of all past observation of <span class="math inline">\((W_t)\)</span>, the next process, called a moving average process of order 1 or MA(1) is (in some sense) a “truncated” version of an AR(1). It is defined as</p>
\begin{equation} 
  X_t = \theta W_{t-1} + W_t,
 \label{eq:defMA1}
\end{equation}
<p>where (again) <span class="math inline">\(W_t\)</span> denotes a Gaussian white noise process. An example on how generate an MA(1) is given below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># This code simulates a gaussian white noise process</span>
n =<span class="st"> </span><span class="dv">100</span>                               <span class="co"># process length</span>
sigma2 =<span class="st"> </span><span class="dv">1</span>                            <span class="co"># innovation variance</span>
theta =<span class="st"> </span><span class="fl">0.5</span>                           <span class="co"># theta parameter</span>
Xt =<span class="st"> </span><span class="kw">gen.gts</span>(<span class="kw">MA1</span>(<span class="dt">theta =</span> theta, <span class="dt">sigma2 =</span> sigma2), <span class="dt">N =</span> n)
<span class="kw">plot</span>(Xt)</code></pre></div>
<p><img src="tts_files/figure-html/example_MA1-1.png" width="672" /></p>
</div>
<div id="linear-drift" class="section level3">
<h3><span class="header-section-number">1.3.5</span> Linear Drift</h3>
<p>A linear drift is a very simple detemrinistic time series model which can be expressed as</p>
<p><span class="math display">\[X_t = X_{t+1} + \omega, \]</span></p>
<p>where <span class="math inline">\(\omega\)</span> is a constant and with the initial condition <span class="math inline">\(X_0 = c\)</span>, an arbitrary constant (typically zero). This process can be expressed in a more familiar form as follows:</p>
<p><span class="math display">\[
  {X_t} = {X_{t - 1}} + \omega 
   = \left( {{X_{t - 2}} + \omega} \right) + \omega 
   = t{\delta} + c  \]</span></p>
<p>Therefore, a (linear) drift corresponds to a simple linear model with slope <span class="math inline">\(\omega\)</span> and intercept <span class="math inline">\(c\)</span>.</p>
<p>A drift can simply be generated used the code below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># This code simulate a linear drift with 0 intercept</span>
n =<span class="st"> </span><span class="dv">100</span>                               <span class="co"># process length</span>
omega =<span class="st"> </span><span class="fl">0.5</span>                           <span class="co"># slope parameter</span>
Xt =<span class="st"> </span><span class="kw">gen.gts</span>(<span class="kw">DR</span>(<span class="dt">omega =</span> omega), <span class="dt">N =</span> n)
<span class="kw">plot</span>(Xt)</code></pre></div>
<p><img src="tts_files/figure-html/example_DR-1.png" width="672" /></p>
</div>
<div id="composite-stochastic-processes" class="section level3">
<h3><span class="header-section-number">1.3.6</span> Composite Stochastic Processes</h3>
<p>A composite stochastic processes can be defined as the sum of underlying (or latent) stochastic processes. In this text, we will use the term <em>latent time series</em> as a synomym to composite stochastic processes. A simple example of such process is for example</p>
<p><span class="math display">\[\begin{aligned}
Y_t &amp;= Y_{t-1} + W_t + \delta\\
X_t &amp;= Y_t + Z_t,
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(W_t\)</span> and <span class="math inline">\(Z_t\)</span> are two independent Gaussian white noise processes. This model often used as first tool to approximate the number of individuals in the context ecological population dynamic. For example, suppose we want to study the popiulation of Chamois in the Swiss Alpes so let <span class="math inline">\(Y_t\)</span> denote the “true” number of individual in this population at time <span class="math inline">\(t\)</span>. It is reasonable that <span class="math inline">\(Y_t\)</span> is (approximately) the population at the previous time <span class="math inline">\(t-1\)</span> (e.g the previous year) plus a random variation and a drift. This random variation is due to the natural random in ecological population and reflects changes in number of predators, in aboundance of food or weather condition. On the other hand, the drift is often of particular interest for ecologist as it can used to determine the “long” term trends for the population (e.g. is the population increasing, stable or decreasing). Of course, <span class="math inline">\(Y_t\)</span> (the number of individauls) is typically unknown and we observed a noisy version of it, denotes as <span class="math inline">\(X_t\)</span>. This process corresponds to the true population plus a measruementr error as some Chamois may not be observed individuals and some counted several times. Interestingly, this process can clearly be expressed as a <em>latent time series model</em> (or composite stochastic process) as follows:</p>
<p><span class="math display">\[\begin{aligned}
R_t &amp;= R_{t-1} + W_t \\
S_t &amp;= \delta t \\
X_t &amp;= R_t + S_t + Z_t,
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(R_t\)</span>, <span class="math inline">\(S_t\)</span> and <span class="math inline">\(Z_t\)</span> denote, respectively, a radnom walk, a drift and a white noise. The code below can be used to simulate such data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">1000</span>                                <span class="co"># process length</span>
delta =<span class="st"> </span><span class="fl">0.005</span>                           <span class="co"># delta parameter (drift)</span>
sigma2 =<span class="st"> </span><span class="dv">10</span>                             <span class="co"># variance parameter (white noise)</span>
gamma2 =<span class="st"> </span><span class="fl">0.1</span>                            <span class="co"># innovation variance (random walk)</span>
model =<span class="st"> </span><span class="kw">WN</span>(<span class="dt">sigma2 =</span> sigma2) +<span class="st"> </span><span class="kw">RW</span>(<span class="dt">gamma2 =</span> gamma2) +<span class="st"> </span><span class="kw">DR</span>(<span class="dt">omega =</span> delta)
Xt =<span class="st"> </span><span class="kw">gen.lts</span>(model, <span class="dt">N =</span> n)
<span class="kw">plot</span>(Xt)</code></pre></div>
<p><img src="tts_files/figure-html/example_ecolo-1.png" width="672" /></p>
<p>In the above graph, the three latent (unobserved) processes are first depicted (i.e. white noise, random walk and drift) and then the sum of the three is present (i.e. <span class="math inline">\((X_t)\)</span>).</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="autocorrelation-and-stationarity.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/SMAC-Group/TTS/edit/master/01-intro.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
