<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Tour of Time Series Analysis with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A Tour of Time Series Analysis with R">
  <meta name="generator" content="bookdown 0.1.16 and GitBook 2.6.7">

  <meta property="og:title" content="A Tour of Time Series Analysis with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tts.smac-group.com/" />
  
  
  <meta name="github-repo" content="SMAC-Group/TTS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Tour of Time Series Analysis with R" />
  
  
  

<meta name="author" content="James Balamuta, Stéphane Guerrier, Roberto Molinari and Haotian Xu">

  
<meta name="date" content="2016-10-11">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="appendixa.html">


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { extensions: ["AMSmath.js"], 
         equationNumbers: { autoNumber: "AMS" },
         Macros: {
          notimplies: "\\nRightarrow",
          real: "\\mathbb{R}",
          integers: "\\mathbb{Z}",
          natural: "\\mathbb{N}",
          rational: "\\mathbb{Q}",
          irrational: "\\mathbb{P}",
          ind: "\\boldsymbol{1}",
          normal: "\\mathcal{N}",
          0: "\\boldsymbol{0}",
          e: ["\\mathbb{E} [#1]",1],
          I: "\\boldsymbol{\\mathbf{I}}",
          S: "\\boldsymbol{S}",
          y: "\\boldsymbol{y}",
          X: "\\boldsymbol{X}",
          C: "\\text{C}",
          btheta: "\\boldsymbol{\\theta}",
          epsilon: "\\varepsilon",
          bbeta: "\\boldsymbol{\\beta}", 
          bepsilon: "\\boldsymbol{\\varepsilon}", 
          norm: "\\mathcal{N}",
          KL: "\\text{KL}",
          AIC: "\\text{AIC}", 
          BIC: "\\text{BIC}", 
          mean: ["\\operatorname{mean}"],
          var: ["\\operatorname{var}"],
          tr: ["\\operatorname{tr}"],
          cov: ["\\operatorname{cov}"],
          corr: ["\\operatorname{corr}"],
          argmax: ["\\operatorname{argmax}"],
          argmin: ["\\operatorname{argmin}"],
          card: ["\\operatorname{card}"],
          diag: ["\\operatorname{diag}"],
          rank: ["\\operatorname{rank}"],
          length: ["\\operatorname{length}"]
    }
  }
});
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="styling/style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tour of Time Series Analysis with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i>Contributing</a></li>
<li class="chapter" data-level="" data-path="bibliographic-note.html"><a href="bibliographic-note.html"><i class="fa fa-check"></i>Bibliographic Note</a></li>
<li class="chapter" data-level="" data-path="rendering-mathematical-formulae.html"><a href="rendering-mathematical-formulae.html"><i class="fa fa-check"></i>Rendering Mathematical Formulae</a></li>
<li class="chapter" data-level="" data-path="r-code-conventions.html"><a href="r-code-conventions.html"><i class="fa fa-check"></i>R Code Conventions</a></li>
<li class="chapter" data-level="" data-path="license.html"><a href="license.html"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>1.1</b> Time Series</a></li>
<li class="chapter" data-level="1.2" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory Data Analysis for Time Series</a></li>
<li class="chapter" data-level="1.3" data-path="basic-time-series-models.html"><a href="basic-time-series-models.html"><i class="fa fa-check"></i><b>1.3</b> Basic Time Series Models</a><ul>
<li class="chapter" data-level="1.3.1" data-path="basic-time-series-models.html"><a href="basic-time-series-models.html#wn"><i class="fa fa-check"></i><b>1.3.1</b> White noise processes</a></li>
<li class="chapter" data-level="1.3.2" data-path="basic-time-series-models.html"><a href="basic-time-series-models.html#rw"><i class="fa fa-check"></i><b>1.3.2</b> Random Walk Processes</a></li>
<li class="chapter" data-level="1.3.3" data-path="basic-time-series-models.html"><a href="basic-time-series-models.html#ar1"><i class="fa fa-check"></i><b>1.3.3</b> Autoregressive Process of Order 1</a></li>
<li class="chapter" data-level="1.3.4" data-path="basic-time-series-models.html"><a href="basic-time-series-models.html#ma1"><i class="fa fa-check"></i><b>1.3.4</b> Moving Average Process of Order 1</a></li>
<li class="chapter" data-level="1.3.5" data-path="basic-time-series-models.html"><a href="basic-time-series-models.html#drift"><i class="fa fa-check"></i><b>1.3.5</b> Linear Drift</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="lts.html"><a href="lts.html"><i class="fa fa-check"></i><b>1.4</b> Composite Stochastic Processes</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html"><i class="fa fa-check"></i><b>2</b> Autocorrelation and Stationarity</a><ul>
<li class="chapter" data-level="2.1" data-path="the-autocorrelation-and-autocovariance-functions.html"><a href="the-autocorrelation-and-autocovariance-functions.html"><i class="fa fa-check"></i><b>2.1</b> The Autocorrelation and Autocovariance Functions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="the-autocorrelation-and-autocovariance-functions.html"><a href="the-autocorrelation-and-autocovariance-functions.html#a-fundamental-representation"><i class="fa fa-check"></i><b>2.1.1</b> A Fundamental Representation</a></li>
<li class="chapter" data-level="2.1.2" data-path="the-autocorrelation-and-autocovariance-functions.html"><a href="the-autocorrelation-and-autocovariance-functions.html#admissible-autocorrelation-functions"><i class="fa fa-check"></i><b>2.1.2</b> Admissible Autocorrelation Functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="stationarity.html"><a href="stationarity.html"><i class="fa fa-check"></i><b>2.2</b> Stationarity</a><ul>
<li class="chapter" data-level="2.2.1" data-path="stationarity.html"><a href="stationarity.html#definitions"><i class="fa fa-check"></i><b>2.2.1</b> Definitions</a></li>
<li class="chapter" data-level="2.2.2" data-path="stationarity.html"><a href="stationarity.html#assessing-weak-stationarity-of-time-series-models"><i class="fa fa-check"></i><b>2.2.2</b> Assessing Weak Stationarity of Time Series Models</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="estimation-of-moments-of-stationary-processes.html"><a href="estimation-of-moments-of-stationary-processes.html"><i class="fa fa-check"></i><b>2.3</b> Estimation of Moments of Stationary Processes</a><ul>
<li class="chapter" data-level="2.3.1" data-path="estimation-of-moments-of-stationary-processes.html"><a href="estimation-of-moments-of-stationary-processes.html#estimation-of-the-mean-function"><i class="fa fa-check"></i><b>2.3.1</b> Estimation of the Mean Function</a></li>
<li class="chapter" data-level="2.3.2" data-path="estimation-of-moments-of-stationary-processes.html"><a href="estimation-of-moments-of-stationary-processes.html#sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>2.3.2</b> Sample Autocovariance and Autocorrelation Functions</a></li>
<li class="chapter" data-level="2.3.3" data-path="estimation-of-moments-of-stationary-processes.html"><a href="estimation-of-moments-of-stationary-processes.html#robustness-issues"><i class="fa fa-check"></i><b>2.3.3</b> Robustness Issues</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="joint-stationarity.html"><a href="joint-stationarity.html"><i class="fa fa-check"></i><b>2.4</b> Joint Stationarity</a><ul>
<li class="chapter" data-level="2.4.1" data-path="joint-stationarity.html"><a href="joint-stationarity.html#sample-cross-covariance-and-cross-correlation-functions"><i class="fa fa-check"></i><b>2.4.1</b> Sample Cross-Covariance and Cross-Correlation Functions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="portmanteau-test.html"><a href="portmanteau-test.html"><i class="fa fa-check"></i><b>2.5</b> Portmanteau test</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-processes.html"><a href="linear-processes.html"><i class="fa fa-check"></i><b>3</b> Linear Processes</a><ul>
<li class="chapter" data-level="3.1" data-path="the-backshift-operator.html"><a href="the-backshift-operator.html"><i class="fa fa-check"></i><b>3.1</b> The Backshift Operator</a><ul>
<li class="chapter" data-level="3.1.1" data-path="the-backshift-operator.html"><a href="the-backshift-operator.html#definition"><i class="fa fa-check"></i><b>3.1.1</b> Definition</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="differencing-operator.html"><a href="differencing-operator.html"><i class="fa fa-check"></i><b>3.2</b> Differencing Operator</a><ul>
<li class="chapter" data-level="3.2.1" data-path="differencing-operator.html"><a href="differencing-operator.html#seasonal-differencing-operator"><i class="fa fa-check"></i><b>3.2.1</b> Seasonal Differencing Operator</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-process.html"><a href="linear-process.html"><i class="fa fa-check"></i><b>3.3</b> Linear Process</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-process.html"><a href="linear-process.html#definition-1"><i class="fa fa-check"></i><b>3.3.1</b> Definition</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-process.html"><a href="linear-process.html#example-white-noise"><i class="fa fa-check"></i><b>3.3.2</b> Example: White Noise</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-process.html"><a href="linear-process.html#example-moving-average-order-1"><i class="fa fa-check"></i><b>3.3.3</b> Example: Moving Average Order 1</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-process.html"><a href="linear-process.html#example-symmetric-moving-average"><i class="fa fa-check"></i><b>3.3.4</b> Example: Symmetric Moving Average</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-process.html"><a href="linear-process.html#example-autoregressive-process-of-order-1"><i class="fa fa-check"></i><b>3.3.5</b> Example: Autoregressive Process of Order 1</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixa.html"><a href="appendixa.html"><i class="fa fa-check"></i><b>A</b> Proofs</a><ul>
<li class="chapter" data-level="A.1" data-path="proof-of-theorem-1.html"><a href="proof-of-theorem-1.html"><i class="fa fa-check"></i><b>A.1</b> Proof of Theorem 1</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/SMAC-Group/TTS" target="blank">&copy; 2016 Balamuta, Guerrier, Molinari, Xu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tour of Time Series Analysis with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="proof-of-theorem-1" class="section level2">
<h2><span class="header-section-number">A.1</span> Proof of Theorem 1</h2>
<p>We let <span class="math inline">\(X_t = W_t + \mu\)</span>, where <span class="math inline">\(\mu &lt; \infty\)</span> and <span class="math inline">\((W_t)\)</span> is a strong white noise process with variance <span class="math inline">\(\sigma^2\)</span> and finite fourth moment (i.e. <span class="math inline">\(\mathbb{E} [W_t^4] &lt; \infty\)</span>).</p>
<p>Next, we consider the sample autocovariance function computed on <span class="math inline">\((X_t)\)</span>, i.e.</p>
<p><span class="math display">\[
\hat \gamma \left( h \right) = \frac{1}{n}\sum\limits_{t = 1}^{n - h} {\left( {{X_t} - \bar X} \right)\left( {{X_{t + h}} - \bar X} \right)}.
\]</span></p>
<p>For this equation, it is clear that <span class="math inline">\(\hat \gamma \left( 0 \right)\)</span> and <span class="math inline">\(\hat \gamma \left( h \right)\)</span> (with <span class="math inline">\(h &gt; 0\)</span>) are two statistics involving sums of different lengths. As we will see, this prevents us from using directly the multivariate central limit theorem on the vector <span class="math inline">\([ \hat \gamma \left( h \right) \;\;\; \hat \gamma \left( h \right) ]^T\)</span>. However, the lag <span class="math inline">\(h\)</span> is fixed and therefore the difference in the number of elements of both sums is asymptotically negligible. Therefore, we define a new statistic</p>
<p><span class="math display">\[\tilde{\gamma} \left( h \right) = \frac{1}{n}\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)},
\]</span></p>
<p>which, as we will see, is easier to used and show that <span class="math inline">\(\hat \gamma \left( h \right)\)</span> and <span class="math inline">\(\tilde{\gamma} \left( h \right)\)</span> are asymptotically equivalent in the sense that:</p>
<p><span class="math display">\[
n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] = o_p(1).
\]</span></p>
<p>Therefore, assuming this results to be true, <span class="math inline">\(\tilde{\gamma} \left( h \right)\)</span> and <span class="math inline">\(\hat \gamma \left( h \right)\)</span> would have the same asymptotic distribution, it is sufficient to show the asymptotic distribution of <span class="math inline">\(\tilde{\gamma} \left( h \right)\)</span>. So that before continuing the proof the Theorem 1 we first state and prove the following lemma:</p>
<p><strong>Lemma A1:</strong> Let</p>
<p><span class="math display">\[
X_t = \mu + \sum\limits_{j = -\infty}^{\infty} \psi_j W_{t-j},
\]</span> where <span class="math inline">\((W_t)\)</span> is a strong white process with variance <span class="math inline">\(\sigma^2\)</span>, and the coefficients satisfying <span class="math inline">\(\sum \, |\psi_j| &lt; \infty\)</span>. Then, we have</p>
<p><span class="math display">\[
n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] = o_p(1).
\]</span></p>
<p><em>Proof:</em> By Markov inequality, we have</p>
<p><span class="math display">\[
\mathbb{P}\left( |n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]| \geq \epsilon \right) \leq \frac{\mathbb{E}|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]|}{\epsilon},
\]</span> for any <span class="math inline">\(\epsilon &gt; 0\)</span>. Thus, it is enough to show that</p>
<p><span class="math display">\[\mathop {\lim }\limits_{n \to \infty } \; \mathbb{E} \left[|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]|\right] = 0\]</span></p>
<p>to prove Lemma A1.By the definitions of <span class="math inline">\(\tilde{\gamma} \left( h \right)\)</span> and <span class="math inline">\(\hat \gamma \left( h \right)\)</span>, we have</p>
<p><span class="math display">\[
\begin{aligned}
n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] &amp;= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n}(X_t - \mu)(X_{t+h} - \mu) \\
&amp;+ \frac{1}{\sqrt{n}} \sum_{t = 1}^{n-h}\left[(X_t - \mu)(X_{t+h} - \mu) - (X_t - \bar{X})(X_{t+h} - \bar{X})\right]\\
&amp;= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n}(X_t - \mu)(X_{t+h} - \mu)  
+ \frac{1}{\sqrt{n}} \sum_{t = 1}^{n-h}\left[(\bar{X} - \mu)(X_t + X_{t+h} - \mu - \bar{X})\right]\\
&amp;= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} (X_t - \mu)(X_{t+h} - \mu) + \frac{1}{\sqrt{n}} (\bar{X} - \mu)\sum_{t = 1}^{n-h}(X_t + X_{t+h} - \mu - \bar{X})\\
&amp;= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} (X_t - \mu)(X_{t+h} - \mu) + \frac{1}{\sqrt{n}} (\bar{X} - \mu)\left[\sum_{t = 1+h}^{n-h}X_t - (n-h)\mu + h\bar{X}\right]\\
&amp;= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} (X_t - \mu)(X_{t+h} - \mu)
+ \frac{1}{\sqrt{n}} (\bar{X} - \mu)\left[\sum_{t = 1+h}^{n-h}(X_t - \mu) - h(\mu - \bar{X})\right]\\
&amp;= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} (X_t - \mu)(X_{t+h} - \mu) + \frac{1}{\sqrt{n}} (\bar{X} - \mu)\sum_{t = 1+h}^{n-h}(X_t - \mu) + \frac{h}{\sqrt{n}} (\bar{X} - \mu)^2,
\end{aligned}
\]</span> where <span class="math inline">\(\bar{X} = \frac{1}{n}\sum_{t=1}^n X_t = \mu + \frac{1}{n}\sum_{t=1}^n\sum_{j=-\infty}^{\infty} \psi_j W_{t-j} = \mu + \frac{1}{n} \sum_{j = -\infty}^{\infty} \sum_{t=1}^n \psi_j W_{t-j}\)</span>.</p>
<p>Then, we have <span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\left|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]\right|\right]
&amp;\leq \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} \mathbb{E}\left[\left|(X_t - \mu) \, (X_{t+h} - \mu)\right|\right]\\
&amp;+ \frac{1}{\sqrt{n}} \mathbb{E} \left[\left|(\bar{X} - \mu) \, \sum_{t = 1+h}^{n-h}(X_t - \mu)\right|\right] +  \frac{h}{\sqrt{n}}\mathbb{E} \left[ (\bar{X} - \mu)^2 \right].
\end{aligned}
\]</span></p>
<p>Next, we consider each term of the above equation. For the first term, since <span class="math inline">\((X_t - \mu)^2 = \left(\sum_{j = -\infty}^{\infty} \psi_j W_{t-j}\right)^2\)</span>, and <span class="math inline">\(\mathbb{E}[W_iW_j] \neq 0\)</span> only if <span class="math inline">\(i = j\)</span>. By Cauchy–Schwarz inequality we have</p>
<p><span class="math display">\[
\mathbb{E}\left[|(X_t - \mu)(X_{t+h} - \mu)|\right] \leq \sqrt{\mathbb{E}\left[|(X_t - \mu)|^2\right] \mathbb{E}\left[|(X_{t+h} - \mu)|^2\right]} = \sigma^2 \sum_{i = -\infty}^{\infty}\psi_i^2.
\]</span></p>
<p>Then, we consider the third term, since it will be used in the second term</p>
<p><span class="math display">\[\mathbb{E}[(\bar{X} - \mu)^2] = \frac{1}{n^2} \sum_{t = 1}^{n} \sum_{i = -\infty}^{\infty} \psi_i^2 \mathbb{E}\left[ W_{t-i}^2 \right] = \frac{\sigma^2}{n} \sum_{i = -\infty}^{\infty}\psi_i^2.\]</span></p>
<p>Similarly, for the second term we have</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left|(\bar{X} - \mu) \sum_{t = 1+h}^{n-h}(X_t - \mu)\right|\right] &amp;\leq \sqrt{\mathbb{E}\left[|(\bar{X} - \mu)|^2\right] \mathbb{E}\left[|\sum_{t = 1+h}^{n-h}(X_t - \mu)|^2\right]}\\
&amp;= \sqrt{\mathbb{E}\left[(\bar{X} - \mu)^2\right] \mathbb{E}\left[\sum_{t = 1+h}^{n-h}\left(X_t - \mu \right)^2 + \sum_{t_1 \neq t_2}(X_{t_1} - \mu)(X_{t_2} - \mu) \right]}\\
&amp;\leq \sqrt{\frac{\sigma^2}{n} \sum_{i = -\infty}^{\infty}\psi_i^2 \cdot (n-2h)\sigma^2 \left( \sum_{j = -\infty}^{\infty} |\psi_j| \right)^2}\\
&amp;\leq \sqrt{\frac{n-2h}{n}}\sigma^2 \left(\sum_{i = -\infty}^{\infty}|\psi_i| \right)^2.
\end{aligned}
\]</span></p>
<p>Combining the above results we obtain</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]|
&amp;\leq \frac{1}{\sqrt{n}} h \sigma^2 \sum_{i = -\infty}^{\infty}\psi_i^2 + \sqrt{\frac{n-2h}{n^2}}\sigma^2 \left(\sum_{i = -\infty}^{\infty}|\psi_i| \right)^2 + \frac{h}{n\sqrt{n}}\sigma^2 \sum_{i = -\infty}^{\infty}\psi_i^2\\
&amp;\leq \frac{1}{n\sqrt{n}} (nh + \sqrt{n - 2h} + h) \sigma^2 \left(\sum_{i = -\infty}^{\infty}|\psi_i|\right)^2,
\end{aligned}
\]</span></p>
<p>By the taking the limit in <span class="math inline">\(n\)</span> we have</p>
<p><span class="math display">\[\mathop {\lim }\limits_{n \to \infty } \; \mathbb{E} \left[|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]|\right] \leq \sigma^2 \left(\sum_{i = -\infty}^{\infty}|\psi_i|\right)^2 \mathop {\lim }\limits_{n \to \infty } \; \frac{nh + \sqrt{n - 2h} + h}{n\sqrt{n}} = 0.
\]</span></p>
<p>We can therefore conclude that</p>
<p><span class="math display">\[\sqrt{n}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] = o_p(1),\]</span></p>
<p>which concludes the proof of Lemma A1. <span class="math inline">\(\;\;\;\;\;\;\;\; \blacksquare\)</span></p>
<p><span class="math inline">\(\\\)</span></p>
<p><span class="math inline">\(\\\)</span></p>
<p>Returning to the proof of Theorem 1, since the process <span class="math inline">\((Y_t)\)</span>, where <span class="math inline">\(Y_t = \left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)\)</span>, is iid, we can apply multivariate central limit theorem to the vector <span class="math inline">\([ \tilde \gamma \left( h \right) \;\;\; \tilde \gamma \left( h \right) ]^T\)</span>, and we obtain</p>
<p><span class="math display">\[\begin{aligned}
    \sqrt{n}\left\{
        \begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix}
    - \mathbb{E}\begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix} \right\} 
    &amp;= \frac{1}{\sqrt{n}}\begin{bmatrix}
         \sum\limits_{t = 1}^{n}(X_t - \mu)^2 - n\mathbb{E}\left[ \tilde{\gamma} \left( 0 \right) \right]\\
         \sum\limits_{t = 1}^{n}\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right) - n\mathbb{E}\left[ \tilde{\gamma} \left( h \right) \right]
        \end{bmatrix} \\
       &amp; \overset{\mathcal{D}}{\to} 
    \mathcal{N}\left(0, n \, \var \left(\begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix} \right)\right)
        \end{aligned}
\]</span></p>
<p>Moreover, by Cauchy–Schwarz inequality and since <span class="math inline">\(\var(X_t) = \sigma^2\)</span>, we have</p>
<p><span class="math display">\[
\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)} \leq \sqrt{\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)^2} \sum\limits_{t = 1}^{n} {\left( {{X_{t + h}} - \mu} \right)^2}} &lt; \infty.
\]</span></p>
<p>Therefore, by bounded convergence theorem and <span class="math inline">\((W_t)\)</span> is iid, we have</p>
<p><span class="math display">\[\begin{aligned}
    \mathbb{E}[\tilde{\gamma} \left( h \right)] &amp;= \frac{1}{n}\mathbb{E}\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)}\right]\\
    &amp;= \frac{1}{n}\left[\sum\limits_{t = 1}^{n} { \mathbb{E}\left( {{X_t} - \mu} \right)\mathbb{E}\left( {{X_{t + h}} - \mu} \right)}\right] =
    \begin{cases}
        \sigma^2, &amp; \text{for } h = 0\\
        0, &amp; \text{for } h \neq 0
    \end{cases}.
    \end{aligned}
\]</span></p>
<p>Next, we consider the variance of <span class="math inline">\(\tilde{\gamma} \left( h \right)\)</span> when <span class="math inline">\(h \neq 0\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
        var[\tilde{\gamma} \left( h \right)] &amp;= \frac{1}{n^2}\mathbb{E}\left\{\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)}\right]^2\right\}\\
        &amp;= \frac{1}{n^2}\mathbb{E}\left\{\left[\sum\limits_{i = 1}^{n} {\left( {{X_i} - \mu} \right)\left( {{X_{i + h}} - \mu} \right)}\right] \left[\sum\limits_{j = 1}^{n} {\left( {{X_j} - \mu} \right)\left( {{X_{j + h}} - \mu} \right)}\right]\right\}\\
        &amp;= \frac{1}{n^2}\mathbb{E}\left[\sum\limits_{i = 1}^{n}\sum\limits_{j = 1}^{n} {\left( {{X_i} - \mu} \right)\left( {{X_{i + h}} - \mu} \right)}{\left( {{X_j} - \mu} \right)\left( {{X_{j + h}} - \mu} \right)}\right].
    \end{aligned}
\]</span></p>
<p>Also by Cauchy–Schwarz inequality and the finite fourth moment assumption, we can use the bounded convergence theorem. Once again since <span class="math inline">\((W_t)\)</span> is white noise process, we have</p>
<p><span class="math display">\[
\mathbb{E}\left[{\left( {{X_i} - \mu} \right)\left( {{X_{i + h}} - \mu} \right)}{\left( {{X_j} - \mu} \right)\left( {{X_{j + h}} - \mu} \right)}\right] \neq 0
\]</span> only when <span class="math inline">\(i = j\)</span>.</p>
<p>Therefore, we obtain</p>
<p><span class="math display">\[\begin{aligned}
        var[\tilde{\gamma} \left( h \right)] &amp;= \frac{1}{n^2}\sum\limits_{i = 1}^{n} \mathbb{E}\left[ {\left( {{X_i} - \mu} \right)^2\left( {{X_{i + h}} - \mu} \right)^2}\right]\\
        &amp;= \frac{1}{n^2}\sum\limits_{i = 1}^{n} \mathbb{E}{\left( {{X_i} - \mu} \right)^2\mathbb{E}\left( {{X_{i + h}} - \mu} \right)^2}
        = \frac{1}{n}\sigma^4.
    \end{aligned}
\]</span></p>
<p>Similarly, for <span class="math inline">\(h = 0\)</span>, we have</p>
<p><span class="math display">\[
\begin{aligned}
        var[\tilde{\gamma} \left( 0 \right)] &amp;= \frac{1}{n^2}\mathbb{E}\left\{\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)^2}\right]^2\right\} - \frac{1}{n^2}\left[\mathbb{E}\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)^2}\right]^2
        = \frac{2}{n}\sigma^4.
    \end{aligned}
\]</span></p>
<p>Next, we consider the covariance between <span class="math inline">\(\tilde{\gamma} \left( 0 \right)\)</span> and <span class="math inline">\(\tilde{\gamma} \left( h \right)\)</span>, for <span class="math inline">\(h \neq 0\)</span>, and we obtain</p>
<p><span class="math display">\[
\begin{aligned}
        cov[\tilde{\gamma} \left( 0 \right), \tilde{\gamma} \left( h \right)] &amp;= \mathbb{E}[\tilde{\gamma} \left( 0 \right) \tilde{\gamma} \left( h \right)] - \mathbb{E}[\tilde{\gamma} \left( 0 \right)] \mathbb{E}[\tilde{\gamma} \left( h \right)]
        = \mathbb{E}[\tilde{\gamma} \left( 0 \right) \tilde{\gamma} \left( h \right)]\\
        &amp;= \mathbb{E}\left[\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)^2}\right]\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)}\right]\right]
        = 0.
    \end{aligned}
\]</span></p>
<p>Therefore by Slutsky’s Theorem we have,</p>
<p><span class="math display">\[
\begin{aligned}
    \sqrt{n}\left\{
        \begin{bmatrix}
         \hat{\gamma} \left( 0 \right) \\
         \hat{\gamma} \left( h \right)
        \end{bmatrix}
    - \begin{bmatrix}
         \sigma^2 \\
         0
        \end{bmatrix} \right\}
    &amp;= \sqrt{n}\left\{
        \begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix}
    - \begin{bmatrix}
         \sigma^2 \\
         0
        \end{bmatrix} \right\}
    + \underbrace{\sqrt{n}\left\{
        \begin{bmatrix}
         \hat{\gamma} \left( 0 \right) \\
         \hat{\gamma} \left( h \right)
        \end{bmatrix}
    - \begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix} \right\}}_{\overset{p}{\to} 0}\\
    &amp;\overset{\mathcal{D}}{\to} 
    \mathcal{N}\left(0, \begin{bmatrix}
         2\sigma^4 &amp; 0\\
         0 &amp; \sigma^4
        \end{bmatrix} \right).
    \end{aligned}
\]</span></p>
<p>Next, we define the function <span class="math inline">\(g\left( \begin{bmatrix}  a \\  b  \end{bmatrix} \right) = b/a\)</span>, where <span class="math inline">\(a \neq 0\)</span>. For this function it is clear that</p>
<p><span class="math display">\[
\nabla g\left( \begin{bmatrix}
         a \\
         b
        \end{bmatrix} \right) = \begin{bmatrix}
         -\frac{b}{a^2} \\
         \frac{1}{a}
        \end{bmatrix}^{T} ,
\]</span></p>
<p>and thus using the Delta method, we have for <span class="math inline">\(h \neq 0\)</span></p>
<p><span class="math display">\[
\begin{aligned}
    \sqrt{n}\hat{\rho}(h) =
    \sqrt{n}\left\{g\left(
        \begin{bmatrix}
         \hat{\gamma} \left( 0 \right) \\
         \hat{\gamma} \left( h \right)
        \end{bmatrix} \right)
    - {\mu} \right\}
    &amp;\overset{\mathcal{D}}{\to} 
    \mathcal{N}\left(0, \sigma_r^2 \right),
    \end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
{\mu} &amp;= g\left(\begin{bmatrix}
         \sigma^2 &amp; 0
        \end{bmatrix} \right) = 0,\\
\sigma_r^2 &amp;= \nabla g\left(\begin{bmatrix}
         \sigma^2 \\
         0
        \end{bmatrix} \right) \begin{bmatrix}
         2\sigma^4 &amp; 0\\
         0 &amp; \sigma^4
        \end{bmatrix} \nabla g\left(\begin{bmatrix}
         \sigma^2 \\
         0
        \end{bmatrix} \right)^{T}
         = \begin{bmatrix}
         0 &amp; \sigma^{-2}
        \end{bmatrix}  \begin{bmatrix}
         2\sigma^4 &amp; 0\\
         0 &amp; \sigma^4
        \end{bmatrix} \begin{bmatrix}
         0 \\
         \sigma^{-2}
        \end{bmatrix} = 1.
    \end{aligned}
\]</span></p>
<p>Thus, we have</p>
<p><span class="math display">\[
\sqrt{n}\hat{\rho}(h) \overset{\mathcal{D}}{\to} 
    \mathcal{N}\left(0, 1 \right),
\]</span></p>
<p>which concludes the proof the Theorem 1. <span class="math inline">\(\;\;\;\;\;\;\;\; \blacksquare\)</span></p>

<div id="refs" class="references">

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="appendixa.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>


<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/SMAC-Group/TTS/edit/master/91-appendix-a.Rmd",
"text": "Edit"
},
"download": ["tts.pdf", "tts.epub", "tts.mobi"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
