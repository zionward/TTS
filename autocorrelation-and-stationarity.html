<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Tour of Time Series Analysis with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A Tour of Time Series Analysis with R">
  <meta name="generator" content="bookdown 0.1.5 and GitBook 2.6.7">

  <meta property="og:title" content="A Tour of Time Series Analysis with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tts.smac-group.com/" />
  
  
  <meta name="github-repo" content="SMAC-Group/TTS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Tour of Time Series Analysis with R" />
  
  
  

<meta name="author" content="James Balamuta, Stéphane Guerrier and Roberto Molinari">

<meta name="date" content="2016-08-18">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction.html">
<link rel="next" href="basic-models.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { extensions: ["AMSmath.js"], 
         equationNumbers: { autoNumber: "AMS" },
         Macros: {
          notimplies: "\\nRightarrow",
          real: "\\mathbb{R}",
          integers: "\\mathbb{Z}",
          natural: "\\mathbb{N}",
          rational: "\\mathbb{Q}",
          irrational: "\\mathbb{P}",
          0: "\\boldsymbol{0}",
          I: "\\boldsymbol{I}",
          S: "\\boldsymbol{S}",
          y: "\\boldsymbol{y}",
          X: "\\boldsymbol{X}",
          epsilon: "\\varepsilon",
          bbeta: "\\boldsymbol{\\beta}", 
          bepsilon: "\\boldsymbol{\\varepsilon}", 
          norm: "\\mathcal{N}",
          KL: "\\text{KL}",
          AIC: "\\text{AIC}", 
          BIC: "\\text{BIC}", 
          mean: ["\\operatorname{mean}"],
          var: ["\\operatorname{var}"],
          tr: ["\\operatorname{tr}"],
          cov: ["\\operatorname{cov}"],
          corr: ["\\operatorname{corr}"],
          argmax: ["\\operatorname{argmax}"],
          argmin: ["\\operatorname{argmin}"],
          card: ["\\operatorname{card}"],
          diag: ["\\operatorname{diag}"],
          rank: ["\\operatorname{rank}"],
          length: ["\\operatorname{length}"]
    }
  }
});
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="styling/style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tour of Time Series Analysis with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bibliographic-note"><i class="fa fa-check"></i>Bibliographic Note</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#rendering-mathematical-formulae"><i class="fa fa-check"></i>Rendering Mathematical Formulae</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#r-code-conventions"><i class="fa fa-check"></i>R Code Conventions</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#time-series"><i class="fa fa-check"></i><b>1.1</b> Time Series</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#exploratory-data-analysis-for-time-series"><i class="fa fa-check"></i><b>1.2</b> Exploratory Data Analysis for Time Series</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#basic-time-series-models"><i class="fa fa-check"></i><b>1.3</b> Basic Time Series Models</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#white-noise-processes"><i class="fa fa-check"></i><b>1.3.1</b> White noise processes</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#random-walk-processes"><i class="fa fa-check"></i><b>1.3.2</b> Random Walk Processes</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#autoregressive-process-of-order-1"><i class="fa fa-check"></i><b>1.3.3</b> Autoregressive Process of Order 1</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#moving-average-process-of-order-1"><i class="fa fa-check"></i><b>1.3.4</b> Moving Average Process of Order 1</a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#linear-drift"><i class="fa fa-check"></i><b>1.3.5</b> Linear Drift</a></li>
<li class="chapter" data-level="1.3.6" data-path="introduction.html"><a href="introduction.html#composite-stochastic-processes"><i class="fa fa-check"></i><b>1.3.6</b> Composite Stochastic Processes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html"><i class="fa fa-check"></i><b>2</b> Autocorrelation and Stationarity</a><ul>
<li class="chapter" data-level="2.1" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#the-autocorrelation-and-autocovariance-functions"><i class="fa fa-check"></i><b>2.1</b> The Autocorrelation and Autocovariance Functions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#definitions"><i class="fa fa-check"></i><b>2.1.1</b> Definitions</a></li>
<li class="chapter" data-level="2.1.2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#a-fundamental-representation"><i class="fa fa-check"></i><b>2.1.2</b> A Fundamental Representation</a></li>
<li class="chapter" data-level="2.1.3" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#admissible-autocorrelation-functions"><i class="fa fa-check"></i><b>2.1.3</b> Admissible autocorrelation functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#stationarity"><i class="fa fa-check"></i><b>2.2</b> Stationarity</a><ul>
<li class="chapter" data-level="2.2.1" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#definitions-1"><i class="fa fa-check"></i><b>2.2.1</b> Definitions</a></li>
<li class="chapter" data-level="2.2.2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#assessing-weak-stationarity-of-time-series-models"><i class="fa fa-check"></i><b>2.2.2</b> Assessing Weak Stationarity of Time Series Models</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#estimation-of-the-mean-function"><i class="fa fa-check"></i><b>2.3</b> Estimation of the Mean Function</a></li>
<li class="chapter" data-level="2.4" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>2.4</b> Sample Autocovariance and Autocorrelation Functions</a></li>
<li class="chapter" data-level="2.5" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html#robustness-issues"><i class="fa fa-check"></i><b>2.5</b> Robustness Issues</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic-models.html"><a href="basic-models.html"><i class="fa fa-check"></i><b>3</b> Basic Models</a><ul>
<li class="chapter" data-level="3.1" data-path="basic-models.html"><a href="basic-models.html#the-backshift-operator"><i class="fa fa-check"></i><b>3.1</b> The Backshift Operator</a></li>
<li class="chapter" data-level="3.2" data-path="basic-models.html"><a href="basic-models.html#white-noise"><i class="fa fa-check"></i><b>3.2</b> White Noise</a></li>
<li class="chapter" data-level="3.3" data-path="basic-models.html"><a href="basic-models.html#moving-average-process-of-order-q-1-a.k.a-ma1"><i class="fa fa-check"></i><b>3.3</b> Moving Average Process of Order q = 1 a.k.a MA(1)</a></li>
<li class="chapter" data-level="3.4" data-path="basic-models.html"><a href="basic-models.html#drift"><i class="fa fa-check"></i><b>3.4</b> Drift</a></li>
<li class="chapter" data-level="3.5" data-path="basic-models.html"><a href="basic-models.html#random-walk"><i class="fa fa-check"></i><b>3.5</b> Random Walk</a></li>
<li class="chapter" data-level="3.6" data-path="basic-models.html"><a href="basic-models.html#random-walk-with-drift"><i class="fa fa-check"></i><b>3.6</b> Random Walk with Drift</a></li>
<li class="chapter" data-level="3.7" data-path="basic-models.html"><a href="basic-models.html#autoregressive-process-of-order-p-1-a.k.a-ar1"><i class="fa fa-check"></i><b>3.7</b> Autoregressive Process of Order p = 1 a.k.a AR(1)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="arma.html"><a href="arma.html"><i class="fa fa-check"></i><b>4</b> ARMA</a><ul>
<li class="chapter" data-level="4.1" data-path="arma.html"><a href="arma.html#definition"><i class="fa fa-check"></i><b>4.1</b> Definition</a></li>
<li class="chapter" data-level="4.2" data-path="arma.html"><a href="arma.html#ma-ar-operators"><i class="fa fa-check"></i><b>4.2</b> MA / AR Operators</a></li>
<li class="chapter" data-level="4.3" data-path="arma.html"><a href="arma.html#redundancy"><i class="fa fa-check"></i><b>4.3</b> Redundancy</a></li>
<li class="chapter" data-level="4.4" data-path="arma.html"><a href="arma.html#causal-invertible"><i class="fa fa-check"></i><b>4.4</b> Causal + Invertible</a></li>
<li class="chapter" data-level="4.5" data-path="arma.html"><a href="arma.html#estimation-of-parameters"><i class="fa fa-check"></i><b>4.5</b> Estimation of Parameters</a><ul>
<li class="chapter" data-level="4.5.1" data-path="arma.html"><a href="arma.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>4.5.1</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="4.5.2" data-path="arma.html"><a href="arma.html#mle-for-sigma-2-on-ar1-with-mean-mu"><i class="fa fa-check"></i><b>4.5.2</b> MLE for <span class="math inline">\(\sigma ^2\)</span> on <span class="math inline">\(AR(1)\)</span> with mean <span class="math inline">\(\mu\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="arma.html"><a href="arma.html#method-of-moments"><i class="fa fa-check"></i><b>4.6</b> Method of Moments</a><ul>
<li class="chapter" data-level="4.6.1" data-path="arma.html"><a href="arma.html#method-of-moments---arp"><i class="fa fa-check"></i><b>4.6.1</b> Method of Moments - AR(p)</a></li>
<li class="chapter" data-level="4.6.2" data-path="arma.html"><a href="arma.html#yule-walker"><i class="fa fa-check"></i><b>4.6.2</b> Yule-Walker</a></li>
<li class="chapter" data-level="4.6.3" data-path="arma.html"><a href="arma.html#estimates"><i class="fa fa-check"></i><b>4.6.3</b> Estimates</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="arma.html"><a href="arma.html#prediction-forecast"><i class="fa fa-check"></i><b>4.7</b> Prediction (Forecast)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/SMAC-Group/TTS" target="blank">&copy; 2016 Balamuta, Guerrier, Molinari</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tour of Time Series Analysis with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="autocorrelation-and-stationarity" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Autocorrelation and Stationarity</h1>
<!-- > "I have seen the future and it is very much like the present, only longer."
>
> --- Kehlog Albran, The Profit -->
<!--
After reading this chapter you will be able to:

- Describe independent and dependent data
- Interpret a processes ACF and CCF.  
- Understand the notion of stationarity.
- Differentiate between Strong and Weak stationarity.
- Judge whether a process is stationary. 

-->
<blockquote>
<p>“<em>One of the first things taught in introductory statistics textbooks is that correlation is not causation. It is also one of the first things forgotten.</em>”, Thomas Sowell</p>
</blockquote>
<p>In this chapter we will discuss and formalize a little how knowledge about <span class="math inline">\(X_{t-1}\)</span> (or <span class="math inline">\(\Omega_t\)</span>) can provide us with some information about <span class="math inline">\(X_t\)</span>. In particular, we will consisder the correlation (or covariance) of <span class="math inline">\((X_t)\)</span> at different times such as <span class="math inline">\(\corr \left(X_t, X_{t+h}\right)\)</span>. This “form” of correlation (covariance) is called the <em>autocorrelation</em> (<em>autocovariance</em>) and is a very usefull tool in time series analysis. Without assuming that a time series present form of “stability”, it would be rather difficult to estimate <span class="math inline">\(\corr \left(X_t, X_{t+h}\right)\)</span> as this quantity would dependent on both <span class="math inline">\(t\)</span> and <span class="math inline">\(h\)</span> leading to far parameters to estimate than observations. Therefore, the concept of <em>stationarity</em> is conveniant in this context as it allow (among other things) to assume that</p>
<p><span class="math display">\[\corr \left(X_t, X_{t+h}\right) = \corr \left(X_t+j, X_{t+h+j}\right),\]</span></p>
<p>implying that the autocorrelation (or autocovariance) is only function of the lag between observation. This two concepts will be discuss in this chapter. Before moving on, it is helpful to remind that correlation (or autocorrelation) is only approriate to measure a very spefic kind on dependence, i.e. linear dependence. There are many forms of dependency as illustrated in the bottom panels on the graph below, which all have a (true) zero correlation:</p>
<div class="figure">
<img src="images/1280px-Correlation_examples2.png" alt="dependency" />
<p class="caption">dependency</p>
</div>
<p>Note that several other metrics have been introduced in the litterature to assess the degree of “dependence” of two random variables but this goes beyond the material discussed in this text.</p>
<!--
However, the methods, covariance and correlation, that we will be using are specific to measuring linear dependence. As a result, these tools are less helpful to measure monotonic dependence and they are much less helpful to measure nonlinearly dependence.

## Dependency



Generally speaking, there is a dependence that within the sequence of random variables.

Recall the difference between independent and dependent data:

*Definition:* **Independence**

$X_1, X_2, \ldots, X_T$ are independent and identically distributed if and only if

\begin{equation}
P\left(X_1 \le x_1, X_2 \le x_2,\ldots, X_{T} \le x_T \right) = P\left(X_1 \le x_1\right) P\left(X_2 \le x_2\right) \cdots P\left(X_{T} \le x_T \right) \label{eq:independent}
\end{equation}

for any $T \ge 2$ and $x_1, \ldots, x_T \in \mathbb{R}$.

*Definition:* **Dependence**

$X_1, X_2, \ldots, X_T$ are identically distributed but dependent, then 

\begin{equation}
\left| {P\left( {{X_1} < {x_1},{X_2} < {x_2}, \ldots ,{X_T} < {x_T}} \right) - P\left( {{X_1} < {x_1}} \right)P\left( {{X_2} < {x_2}} \right) \cdots P\left( {{X_T} < {x_T}} \right)} \right| \ne 0 \label{eq:dependent}
\end{equation}

for some $x_1, \ldots, x_T \in \mathbb{R}$.

### Measuring (Linear) Dependence

There are many forms of dependency...

![dependency](images/1280px-Correlation_examples2.png)

However, the methods, covariance and correlation, that we will be using are specific to measuring linear dependence. As a result, these tools are less helpful to measure monotonic dependence and they are much less helpful to measure nonlinearly dependence.

-->
<div id="the-autocorrelation-and-autocovariance-functions" class="section level2">
<h2><span class="header-section-number">2.1</span> The Autocorrelation and Autocovariance Functions</h2>
<!--
Dependence between $T$ different RV is difficult to measure in one shot! So we consider just two random variables, $X_t$ and $X_{t+h}$. Then one
(linear) measure of dependence is the covariance between $\left(X_t , X_{t+h}\right)$. Since $X$ is the same RV observed at two different time
points, the covariance between $X_t$ and $X_{t+h}$ is defined as the Autocovariance.
-->
<div id="definitions" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Definitions</h3>
<p>The <em>autocovariance function</em> of a series <span class="math inline">\((X_t)\)</span> is defined as</p>
<p><span class="math display">\[{\gamma_x}\left( {t,t+h} \right) = \operatorname{cov} \left( {{x_t},{x_{t+h}}} \right).\]</span></p>
<p>Since we generally consider stochastic processes with constant zero mean we often have</p>
<p><span class="math display">\[{\gamma_x}\left( {t,t+h} \right) = E\left[X_t X_{t+h} \right]. \]</span></p>
<!--  The notation used above corresponds to:

\[\begin{aligned}
  \operatorname{cov} \left( {{X_t},{X_{t+h}}} \right) &= E\left[ {{X_t}{X_{t+h}}} \right] - E\left[ {{X_t}} \right]E\left[ {{X_{t+h}}} \right] \\
  E\left[ {{X_t}} \right] &= \int\limits_{ - \infty }^\infty  {x \cdot {f_x}\left( x \right)dx}  \\
  E\left[ {{X_t}{X_{t+h}}} \right] &= \int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {{x_1}{x_2} \cdot f\left( {{x_1},{x_2}} \right)d{x_1}d{x_2}} }  \\ 
\end{aligned} \] -->
<p>We normally drop the subscript referring to the time series if it is clear to the time series the autocovariance function is referencing. For example, we generally use <span class="math inline">\({\gamma}\left( {t,t+h} \right)\)</span> instead of <span class="math inline">\({\gamma_x}\left( {t,t+h} \right)\)</span>. Moreover, the notation is even further simplify when the covariance of <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+h}\)</span> is the same as that of <span class="math inline">\(X_{t+j}\)</span> and <span class="math inline">\(X_{t+h+j}\)</span> (for <span class="math inline">\(j \in \mathbb{Z}\)</span>), i.e. that the covariance depends only on the time between observations and not the absolute date <span class="math inline">\(t\)</span>. This is an important property call <em>stationarity</em>, which will be discuss in the next section. In this case, we simply use to following notation: <span class="math display">\[\gamma \left( {h} \right) = \operatorname{cov} \left( X_t , X_{t+h} \right). \]</span></p>
<p>A few other remarks:</p>
<ol style="list-style-type: decimal">
<li>The covariance function is <strong>symmetric</strong>. That is, <span class="math inline">\({\gamma}\left( {h} \right) = {\gamma}\left( -h \right)\)</span> since <span class="math inline">\(\operatorname{cov} \left( {{X_t},{X_{t+h}}} \right) = \operatorname{cov} \left( X_{t+h},X_{t} \right)\)</span>.</li>
<li>Note that <span class="math inline">\(\operatorname{var} \left( X_{t} \right) = {\gamma}\left( 0 \right)\)</span>.</li>
<li>We have that <span class="math inline">\(|\gamma(h)| \leq \gamma(0)\)</span> for all <span class="math inline">\(h\)</span>. The proof of this inequality follows from Cauchy-Schwarz inequality, i.e. <span class="math display">\[ \begin{aligned}
\left(|\gamma(h)| \right)^2 &amp;= \gamma(h)^2 = \left(E\left[\left(X_t - E[X_t] \right)\left(X_{t+h} - E[X_{t+h}] \right)\right]\right)^2\\
&amp;\leq E\left[\left(X_t - E[X_t] \right)^2 \right] E\left[\left(X_{t+h} - E[X_{t+h}] \right)^2 \right] =  \gamma(0)^2. 
\end{aligned}
\]</span></li>
<li>Just as any covariance, the <span class="math inline">\({\gamma}\left( {h} \right)\)</span> is “scale dependent”, <span class="math inline">\({\gamma}\left( {h} \right) \in \real\)</span>, or <span class="math inline">\(-\infty \le {\gamma}\left( {h} \right) \le +\infty\)</span>, so of course we have:
<ul>
<li>if <span class="math inline">\(\left| {\gamma}\left( {h} \right) \right|\)</span> is “close” to zero, then <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+h}\)</span> are “weakly” (linearly) dependent,</li>
<li>if <span class="math inline">\(\left| {\gamma}\left( {h} \right) \right|\)</span> is “far” from zero, then the two random variable present a “strong” (linear) dependence, but this is generally difficult to asses what “close” and “far” from zero means in this case.</li>
</ul></li>
<li><span class="math inline">\({\gamma}\left( {h} \right)=0\)</span> does not imply <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+h}\)</span> are independent. This is only true in joint Gaussian case.</li>
</ol>
<p>An important related statistic is the correlation of <span class="math inline">\(X_t\)</span> with <span class="math inline">\(X_{t+h}\)</span> or <em>autocorrelation</em> which is defined as</p>
<p><span class="math display">\[\rho \left(  h \right) = \operatorname{corr}\left( {{X_t},{X_{t + h}}} \right) = \frac{\gamma(h) }{\gamma(0)}.\]</span></p>
<!-- The more commonly used formulation for weakly stationary processes (more next section) is:
\[\rho \left( {{X_t},{X_{t + h}}} \right) = \frac{{Cov\left( {{X_t},{X_{t + h}}} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{X_{t + h}}}}}} = \frac{{\gamma \left( h \right)}}{{\gamma \left( 0 \right)}} = \rho \left( h \right)\] -->
<p>Similarly to <span class="math inline">\(\gamma(h)\)</span>, it is important to note that the above notation implies that the autocorrelation function is only a function of the lag <span class="math inline">\(h\)</span> between observations. Thus, autocovariances and autocorrelations are one possible way to describe the joint distribution of a time series. Indeed, the correlation of <span class="math inline">\(X_t\)</span> with <span class="math inline">\(X_{t+1}\)</span> is an obvious measure of how <em>persistent</em> a time series is.</p>
<p>Remeber that just as with any correlation:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\rho \left( h \right)\)</span> is scale free so it is much easier to interpret that than <span class="math inline">\(\gamma(h)\)</span>. <!-- 2. $\rho \left( {{X_t},{X_{t + h}}} \right)$ is closer to $\pm 1 \Rightarrow \left({ X_t, X_{t+h} } \right)$ "more dependent." --></li>
<li><span class="math inline">\(|\rho \left( h \right)| \leq 1\)</span> since <span class="math inline">\(|\gamma(h)| \leq \gamma(0)\)</span>.</li>
<li>Causation and correlation are two very different things!</li>
</ol>
</div>
<div id="a-fundamental-representation" class="section level3">
<h3><span class="header-section-number">2.1.2</span> A Fundamental Representation</h3>
<p>Autocovariances and autocorrelation also turn out to be a very useful tool because they are one of the <em>fundamental representations</em> of time series. Indeed, if we consider a zero mean normally distrbuted process it is clear that its joint distribution is fully characterized by the autocariances <span class="math inline">\(E[X_t X_{t+h}]\)</span> (since the joint probability density only depends of these covariances). Once we know the autocovariances we know <em>everything</em> there is to know about the process and therefore: <em>if two processes have the same autocovariance function, then they are the same process.</em></p>
</div>
<div id="admissible-autocorrelation-functions" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Admissible autocorrelation functions</h3>
<p>Since the autocorrelation is related to a fundamental representation of time series it implies that one might be able to define a stochastic process by picking a set autocorrelation values. However, it turns out not every collection of numbers such as <span class="math inline">\(\{\rho_1, \rho_2, ...\}\)</span> is the autocorrelation of a process. Two conditions are required to ensure the validity of an autocorrelation sequence:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\operatorname{max}_j \; | \rho_j| \leq 1\)</span>.</li>
<li><span class="math inline">\(\operatorname{var} \left[\sum_{j = 0}^\infty \alpha_j X_{t-j} \right] \geq 0\)</span> for all <span class="math inline">\(\{\alpha_0, \alpha_1, ...\}\)</span>.</li>
</ol>
<p>The first condition is obvious and simply relects the fact that <span class="math inline">\(|\rho \left( h \right)| \leq 1\)</span> but the second is more difficult to verify. Let <span class="math inline">\(\alpha_j = 0, \; j &gt; 1\)</span>, then conditon 2 implies that</p>
<p><span class="math display">\[\operatorname{var} \left[ \alpha_0 X_{t} + \alpha_1 X_{t-1}  \right] = \gamma_0 \begin{bmatrix}
  \alpha_0 &amp; \alpha_1
 \end{bmatrix}   \begin{bmatrix}
  1 &amp; \rho_1\\
  \rho_1 &amp; 1
 \end{bmatrix} \begin{bmatrix}
  \alpha_0 \\
  \alpha_1
 \end{bmatrix} \geq 0. \]</span></p>
<p>Thus, the matrix</p>
<p><span class="math display">\[ \boldsymbol{A}_1 = \begin{bmatrix}
  1 &amp; \rho_1\\
  \rho_1 &amp; 1
 \end{bmatrix} \]</span></p>
<p>must be positive semi-definite. Therefore,</p>
<p><span class="math display">\[\operatorname{det} \left(\boldsymbol{A}_1\right) = 1 - \rho_1^2 \]</span></p>
<p>implying that <span class="math inline">\(|\rho_1| &lt; 1\)</span>. Next, let <span class="math inline">\(\alpha_j = 0, \; j &gt; 2\)</span>, then we must verify that:</p>
<p><span class="math display">\[\operatorname{var} \left[ \alpha_0 X_{t} + \alpha_1 X_{t-1}  + \alpha_2 X_{t-2} \right] = \gamma_0 \begin{bmatrix}
  \alpha_0 &amp; \alpha_1 &amp;\alpha_2
 \end{bmatrix}   \begin{bmatrix}
  1 &amp; \rho_1 &amp; \rho_2\\
  \rho_1 &amp; 1 &amp; \rho_1 \\
  \rho_2 &amp; \rho_1 &amp; 1
 \end{bmatrix} \begin{bmatrix}
  \alpha_0 \\
  \alpha_1 \\
  \alpha_2
 \end{bmatrix} \geq 0. \]</span></p>
<p>Similarly, this implies that the matrix</p>
<p><span class="math display">\[ \boldsymbol{A}_2 = \begin{bmatrix}
  1 &amp; \rho_1 &amp; \rho_2\\
  \rho_1 &amp; 1 &amp; \rho_1 \\
  \rho_2 &amp; \rho_1 &amp; 1
 \end{bmatrix} \]</span></p>
<p>must be positive semi-definite. It is easy to verify that</p>
<p><span class="math display">\[\operatorname{det} \left(\boldsymbol{A}_2\right) = \left(1 - \rho_2 \right)\left(- 2 \rho_1^2 + \rho_2 + 1\right). \]</span></p>
<p>It implies that <span class="math inline">\(|\rho_2| &lt; 1\)</span> as well as</p>
<p><span class="math display">\[\begin{aligned} &amp;- 2 \rho_1^2 + \rho_2 + 1 \geq 0 \Rightarrow 1 &gt; \rho_2 \geq 2 \rho_1^2 - 1 \\
&amp;\Rightarrow 1 - \rho_1^2 &gt; \rho_2 - \rho_1^2 \geq -(1 - \rho_1^2)\\
&amp;\Rightarrow 1 &gt; \frac{\rho_2 - \rho_1^2 }{1 - \rho_1^2} \geq -1,
\end{aligned}\]</span></p>
<p>imlying that <span class="math inline">\(\rho_1\)</span> and <span class="math inline">\(\rho_2\)</span> must lie in a parabolic shaped region defined by the above inequalities as illustrated in the figure below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="ot">NA</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(-<span class="fl">1.1</span>,<span class="fl">1.1</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(-<span class="fl">1.1</span>,<span class="fl">1.1</span>), <span class="dt">xlab =</span> <span class="kw">expression</span>(rho[<span class="dv">1</span>]),
     <span class="dt">ylab =</span> <span class="kw">expression</span>(rho[<span class="dv">2</span>]), <span class="dt">cex.lab =</span> <span class="fl">1.5</span>)
<span class="kw">grid</span>()

<span class="co"># Adding boundary of constraint |rho_1| &lt; 1</span>
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">c</span>(-<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkgrey&quot;</span>)

<span class="co"># Adding boundary of constraint |rho_2| &lt; 1</span>
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="kw">c</span>(-<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkgrey&quot;</span>)

<span class="co"># Adding boundary of non-linear constraint</span>
rho1 =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> -<span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">10</span>^<span class="dv">3</span>)
rho2 =<span class="st"> </span>(rho1^<span class="dv">2</span> -<span class="st"> </span><span class="dv">1</span>) +<span class="st"> </span>rho1^<span class="dv">2</span> 
<span class="kw">lines</span>(rho1, rho2, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkgrey&quot;</span>)

<span class="co"># Adding admissible region</span>
<span class="kw">polygon</span>(<span class="kw">c</span>(rho1,<span class="kw">rev</span>(rho1)),<span class="kw">c</span>(rho2,<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">10</span>^<span class="dv">3</span>)),
        <span class="dt">border =</span> <span class="ot">NA</span>, <span class="dt">col=</span> <span class="kw">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>))

<span class="co"># Adding text</span>
<span class="kw">text</span>(<span class="dv">0</span>,<span class="dv">0</span>, <span class="kw">c</span>(<span class="st">&quot;Admissible Region&quot;</span>))</code></pre></div>
<p><img src="tts_files/figure-html/admissibility-1.png" width="672" /></p>
<p>Therefore, the restrictions on the autocorrelation are very complicated providing a motivation for other form of fundamental representation, which will explore later in this text. Before moving on the estimation of the autocorrelation and covariance function we first discuss the stationarity of <span class="math inline">\((X_t)\)</span>, which will provide a conveninant framework in which <span class="math inline">\(\gamma(h)\)</span> and <span class="math inline">\(\rho(h)\)</span> can be used (rather that <span class="math inline">\(\gamma(t,t+h)\)</span> for example).</p>
<!-- Remember... When using correlation....

![correlation_sillies](images/correlation-does-not-imply-causation.jpg) -->
<!-- #### Cross dependency functions

Consider two time series, say $\left(X_t \right)$ and $\left(Y_t \right)$. Then, the cross-covariance function between two series $\left(X_t \right)$ and $\left(Y_t \right)$ is:

\[{\gamma _{XY}}\left( {t,t + h} \right) = \operatorname{cov} \left( {{X_t},{Y_{t + h}}} \right) = E\left[ {\left( {{X_t} - E\left[ {{X_t}} \right]} \right)\left( {{Y_{t + h}} - E\left[ {{Y_{t + h}}} \right]} \right)} \right].\]

The cross-correlation function is given by
\[{\rho _{XY}}\left( {t,t + h} \right) = Corr\left( {{X_t},{Y_{t + h}}} \right) = \frac{{{\gamma _{XY}}\left( {t,t + h} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{Y_{t + h}}}}}}.\] 

These ideas can extended beyond the bivariate case to a general multivariate setting. -->
</div>
</div>
<div id="stationarity" class="section level2">
<h2><span class="header-section-number">2.2</span> Stationarity</h2>
<div id="definitions-1" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Definitions</h3>
<p>There are two kinds of stationarity which are commonly used. They are defined below:</p>
<ul>
<li>A process <span class="math inline">\((X_t)\)</span> is <em>strongly stationary</em> or <em>strictly stationary</em> if the joint probability distribution of <span class="math inline">\(\{X_{t-h}, ..., X_t, ..., X_{t+h}\}\)</span> is independent of <span class="math inline">\(t\)</span> for all <span class="math inline">\(h\)</span>.</li>
<li>A process <span class="math inline">\((X_t)\)</span> is <em>weakly stationary</em>, <em>covariance stationary</em> or <em>second order stationary</em> if <span class="math inline">\(E[X_t]\)</span>, <span class="math inline">\(E[X_t^2]\)</span> are finite and <span class="math inline">\(E[X_t X_{t-h}]\)</span> depends only on <span class="math inline">\(h\)</span> and not on <span class="math inline">\(t\)</span>.</li>
</ul>
<p>These types of stationarity are <em>not equivalent</em> and the presence of one kind of stationarity does not imply the other. That is, a time series can be strongly stationary but not weakly stationary and vice versa. In some cases, a time series can be both strong and weakly stationary, this is happends for example in the (joint) Gaussian case. Stationarity of <span class="math inline">\((X_t)\)</span> matters, because <em>it provides the framework in which averaging dependent data makes sense</em> allows to easily estimate quantities such as the autocorrelation function.</p>
<p>A few remarks:</p>
<ul>
<li>Strong stationarity <em>does not imply</em> weak stationarity. <em>Example</em>: an iid Cauchy process is strongly but not weakly stationary.</li>
<li>Weak stationarity <em>does not imply</em> strong stationarity. <em>Example</em>: Consider the following weak white noise process: <span class="math inline">\(X_{2t} = U_{2t}, X_{2t+1} = V_{2t+1}\)</span>, for <span class="math inline">\(t = 1,..., n\)</span> where <span class="math inline">\({U_t}\mathop \sim \limits^{iid} N\left( {1,1} \right)\)</span> and <span class="math inline">\({V_t}\mathop \sim \limits^{iid} Exponential\left( 1 \right)\)</span> is weakly stationary but <em>not</em> strongly stationary.</li>
<li>Strong stationarity combined with bounded values of <span class="math inline">\(E[X_t]\)</span> and <span class="math inline">\(E[X_t^2]\)</span> <em>implies</em> weak stationarity</li>
<li>Weak stationarity combined with normalityof the process <em>implies</em> strong stationarity.</li>
</ul>
<!-- With this being said, here are a few examples of stationarity:

1. $X_t \sim Cauchy$ is strictly stationary but **NOT** weakly stationary.
    * The strong stationarity exists due to the symmetric properties of the distribution.
    * It cannot be weakly stationary because it has an infinite variance!
2. $X_{2t} = U_{2t}, X_{2t+1} = V_{2t+1} \forall t$ where ${U_t}\mathop \sim \limits^{iid} N\left( {1,1} \right)$ and ${V_t}\mathop \sim \limits^{iid} Exponential\left( 1 \right)$ is weakly stationary but **NOT** strictly stationary.
    * The weak stationary exists since the mean is constant ($\mu = 1$) and the variance does not depend on time ($\sigma ^2 = 1$).
    * It cannot be strongly stationary due to values not aligning in time. 


Regarding white noises, we  can obtain different levels of stationarity depending on the assumption:

1. If $X_t \sim WN$, e.g. **uncorrelated observations** with a finite variance, then it is weakly stationary but **NOT** strictly stationary.
2. If $X_t \mathop \sim \limits^{iid} NWN$, e.g. **normally distributed independent observations** with a finite variance, then it is weakly stationary *AND* strictly stationary.

The autocovariance of weakly stationary processes has the following properties:

1. $\gamma \left(0\right) = var\left[X_t \right] \ge 0$ (variance) 
2. $\gamma \left(h\right) = \gamma \left(-h\right)$ (function is even / symmetric)
3. $\left| \gamma \left(h\right) \right| \le \gamma \left( 0 \right) \forall h.$

We obtain these properties through:

1. \[\gamma \left( 0 \right) = Var\left( {{x_t}} \right) = E\left[ {{{\left( {{x_t} - \mu } \right)}^2}} \right] = \sum\limits_{t = 1}^T {{p_t}{{\left( {{x_t} - \mu } \right)}^2}}  = {p_1}{\left( {{x_1} - \mu } \right)^2} +  \cdots  + {p_T}{\left( {{x_T} - \mu } \right)^2} \ge 0\]
2. \[\begin{aligned}
  \gamma \left( h \right) &= \gamma \left( {t + h - t} \right) \\
   &= E\left[ {\left( {{x_{t + h}} - \mu } \right)\left( {{x_t} - \mu } \right)} \right] \\
   &= E\left[ {\left( {{x_t} - \mu } \right)\left( {{x_{t + h}} - \mu } \right)} \right] \\
   &= \gamma \left( {t - \left( {t + h} \right)} \right) \\
   &= \gamma \left( { - h} \right) 
\end{aligned}\]
3. Using the Cauchy-Schwarz Inequality, ${\left( {E\left[ {XY} \right]} \right)^2} \le E\left[ {{X^2}} \right]E\left[ {{Y^2}} \right]$, we have:
\[\begin{aligned}
  {\left( {\left| {\gamma \left( h \right)} \right|} \right)^2} &= {\left( {\gamma \left( h \right)} \right)^2}  \\
   &= {\left( {E\left[ {\left( {{x_t} - \mu } \right)\left( {{x_{t + h}} - \mu } \right)} \right]} \right)^2}  \\
   &\le E\left[ {{{\left( {{x_t} - \mu } \right)}^2}} \right]E\left[ {{{\left( {{x_{t + h}} - \mu } \right)}^2}} \right]  \\
   &= {\left( {\gamma \left( 0 \right)} \right)^2}  \\
  {\left( {\gamma \left( h \right)} \right)^2} &\le {\left( {\gamma \left( 0 \right)} \right)^2}  \\
  \left| {\gamma \left( h \right)} \right| &\le \gamma \left( 0 \right) 
\end{aligned}\]

-->
</div>
<div id="assessing-weak-stationarity-of-time-series-models" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Assessing Weak Stationarity of Time Series Models</h3>
<p>It is important to understand how to verify if a postulated model is (waekly) stationary. In order to do so, we must ensure that the model satisfies three properties, i.e.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E\left[X_t \right] = \mu_t = \mu &lt; \infty\)</span>,</li>
<li><span class="math inline">\(\operatorname{var}\left[X_t \right] = \sigma^2_t = \sigma^2 &lt; \infty\)</span>,</li>
<li><span class="math inline">\(\operatorname{cov}\left(X_t, X_{t+h} \right) = \gamma \left(h\right)\)</span>.</li>
</ol>
<p>In the following examples we evaluate the stationarity of the processes introduced in Section <a href="introduction.html#basic-time-series-models">1.3</a>.</p>
<p><strong>Example: Gaussian White Noise</strong> It is easy to verify that a this process is stationary. Indeed, we have:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E\left[ {{X_t}} \right] = 0\)</span>,</li>
<li><span class="math inline">\(\gamma(0) = \sigma^2 &lt; \infty\)</span>,<br />
</li>
<li><span class="math inline">\(\gamma(h) = 0\)</span> for <span class="math inline">\(|h| &gt; 0\)</span>.</li>
</ol>
<p><strong>Example: Random Walk</strong> To evaluate the stationarity of this process we first derive its properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math display">\[\begin{aligned}
  E\left[ {{X_t}} \right] &amp;= E\left[ {{X_{t - 1}} + {W_t}} \right] 
   = E\left[ {\sum\limits_{i = 1}^t {{W_t}}  + {X_0}} \right] \\
   &amp;= E\left[ {\sum\limits_{i = 1}^t {{W_t}} } \right] + {c} 
   = c \\ 
\end{aligned} \]</span> Note that the mean here is constant since it depends only on the value of the first term in the sequence.</li>
<li><p><span class="math display">\[\begin{aligned}
  \operatorname{var}\left( {{X_t}} \right) &amp;= \operatorname{var}\left( {\sum\limits_{i = 1}^t {{W_t}}  + {X_0}} \right) 
   = \operatorname{var}\left( {\sum\limits_{i = 1}^t {{w_t}} } \right) + \underbrace {\operatorname{var}\left( {{X_0}} \right)}_{= 0} \\
   &amp;= \sum\limits_{i = 1}^t {Var\left( {{w_t}} \right)} 
   = t \sigma_w^2. 
\end{aligned}\]</span> where <span class="math inline">\(\sigma_w^2 = \operatorname{var}(W_t)\)</span>. Therefore, the variance has a dependence on time contradicting our second property. Moreover, we have: <span class="math display">\[\mathop {\lim }\limits_{t \to \infty } \; \operatorname{var}\left(X_t\right) = \infty.\]</span> This process is therefore not weakly stationary.</p></li>
<li><p>Rgearding the autovariance of a random walk we have: <span class="math display">\[\begin{aligned}
  \gamma \left( h \right) &amp;= Cov\left( {{X_t},{X_{t + h}}} \right) 
   = Cov\left( {\sum\limits_{i = 1}^t {{W_i}} ,\sum\limits_{j = 1}^{t + h} {{W_j}} } \right) \\
   &amp;= Cov\left( {\sum\limits_{i = 1}^t {{W_i}} ,\sum\limits_{j = 1}^t {{W_j}} } \right) 
   = \min \left( {t,t + h} \right)\sigma _w^2 \\
   &amp;= \left( {t + \min \left( {0,h} \right)} \right)\sigma _w^2,
\end{aligned} \]</span></p></li>
</ol>
<p>which further illustrates that non-stationarity of this process.</p>
<p>Moreover, the autocorrelation of this process is given by</p>
<p><span class="math display">\[\rho (h) = \frac{t + \min \left( {0,h} \right)}{\sqrt{t}\sqrt{t+h}},\]</span></p>
<p>implying (for a fixed <span class="math inline">\(h\)</span>) that</p>
<p><span class="math display">\[\mathop {\lim }\limits_{t \to \infty } \; \rho(h) = 1.\]</span></p>
<p>In the following simulated example, we illustrate the non-stationary feature of such process:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># In this example, we simulate a large number of random walks</span>
<span class="co"># Number of simulated processes</span>
B =<span class="st"> </span><span class="dv">200</span>

<span class="co"># Length of random walks</span>
n =<span class="st"> </span><span class="dv">1000</span>

<span class="co"># Output matrix</span>
out =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,B,n)

for (i in <span class="dv">1</span>:B){
  <span class="co"># Simulate random walk</span>
  Xt =<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">rnorm</span>(n))
  
  <span class="co"># Store process</span>
  out[i,] =<span class="st"> </span>Xt
}

<span class="co"># Plot random walks</span>
<span class="kw">plot</span>(<span class="ot">NA</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1</span>,n), <span class="dt">ylim =</span> <span class="kw">range</span>(out), <span class="dt">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot; &quot;</span>)
color =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">topo.colors</span>(B, <span class="dt">alpha =</span> <span class="fl">0.5</span>))
for (i in <span class="dv">1</span>:B){
  <span class="kw">lines</span>(out[i,], <span class="dt">col =</span> color[i])
}

<span class="co"># Add 95% confidence region</span>
<span class="kw">lines</span>(<span class="dv">1</span>:n, <span class="fl">1.96</span>*<span class="kw">sqrt</span>(<span class="dv">1</span>:n), <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">lines</span>(<span class="dv">1</span>:n, -<span class="fl">1.96</span>*<span class="kw">sqrt</span>(<span class="dv">1</span>:n), <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="tts_files/figure-html/RW-1.png" width="672" /></p>
<p>The relationship between time and variance can clearly be observed in the above graph.</p>
<p><strong>Example: MA(1)</strong> Similarly to our previous examples, we attempt to verify the stationary properties for the MA(1) model defined in \ref{eq:defMA1} <strong>REF NOT WORKING</strong>:</p>
<ol style="list-style-type: decimal">
<li><span class="math display">\[ 
  E\left[ {{X_t}} \right] = E\left[ {{\theta_1}{W_{t - 1}} + {W_t}} \right] 
   = {\theta_1}E\left[ {{W_{t - 1}}} \right] + E\left[ {{W_t}} \right] 
   = 0. \]</span></li>
<li><span class="math display">\[\var \left( {{X_t}} \right) = \theta_1^2 \var \left( W_{t - 1}\right) + \var \left( W_{t}\right) = \left(1 + \theta^2 \right) \sigma^2_w.\]</span><br />
</li>
<li><span class="math display">\[\begin{aligned}
  Cov\left( {{X_t},{X_{t + h}}} \right) &amp;= E\left[ {\left( {{X_t} - E\left[ {{X_t}} \right]} \right)\left( {{X_{t + h}} - E\left[ {{X_{t + h}}} \right]} \right)} \right] = E\left[ {{X_t}{X_{t + h}}} \right] \\
   &amp;= E\left[ {\left( {{\theta}{W_{t - 1}} + {W_t}} \right)\left( {{\theta }{W_{t + h - 1}} + {W_{t + h}}} \right)} \right] \\
   &amp;= E\left[ {\theta^2{W_{t - 1}}{W_{t + h - 1}} + \theta {W_t}{W_{t + h}} + {\theta}{W_{t - 1}}{W_{t + h}} + {W_t}{W_{t + h}}} \right]. \\
  \end{aligned} \]</span></li>
</ol>
<p>It is easy to see that <span class="math inline">\(E\left[ {{W_t}{W_{t + h}}} \right] = {\boldsymbol{1}_{\left\{ {h = 0} \right\}}}\sigma _w^2\)</span> and therefore, we obtain</p>
<p><span class="math display">\[\cov \left( {{X_t},{X_{t + h}}} \right) = \left( {\theta^2{ \boldsymbol{1}_{\left\{ {h = 0} \right\}}} + {\theta}{\boldsymbol{1}_{\left\{ {h = 1} \right\}}} + {\theta}{\boldsymbol{1}_{\left\{ {h =  - 1} \right\}}} + {\boldsymbol{1}_{\left\{ {h = 0} \right\}}}} \right)\sigma _w^2\]</span></p>
<p>implying the following autocovariance function:</p>
<p><span class="math display">\[\gamma \left( h \right) = \left\{ {\begin{array}{*{20}{c}}
  {\left( {\theta^2 + 1} \right)\sigma _w^2}&amp;{h = 0} \\ 
  {{\theta}\sigma _w^2}&amp;{\left| h \right| = 1} \\ 
  0&amp;{\left| h \right| &gt; 1} 
  \end{array}} \right. .\]</span></p>
<p><!--  \\
   \\
   &\Rightarrow 
 
 
--> Therefore, an MA(1) process is weakly stationary since both the mean and variance are constant over time and its covariance function is only a function of the lag <span class="math inline">\(h\)</span>. Finally, we can easily obtain the autocorrelation for this process, which is given by</p>
<p><span class="math display">\[\Rightarrow \rho \left( h \right) = \left\{ {\begin{array}{*{20}{c}}
  1&amp;{h = 0} \\ 
  {\frac{{{\theta}\sigma _w^2}}{{\left( {\theta^2 + 1} \right)\sigma _w^2}} = \frac{{{\theta}}}{{\theta^2 + 1}}}&amp;{\left| h \right| = 1} \\ 
  0&amp;{\left| h \right| &gt; 1} 
\end{array}} \right.\]</span></p>
<p>Interestingly, we can note that <span class="math inline">\(|\rho(1)| \leq 0.5\)</span>.</p>
<p><strong>Example AR(1)</strong></p>
<p><strong>JAMES TO DO - USE MA(1) AS EXAMPLE, ADD DETAILS FROM HOMEWORK, CHANGE <span class="math inline">\(\phi_1\)</span> in <span class="math inline">\(\phi\)</span> and add ref to chap 1. Thanks</strong></p>
<p>Consider the AR(1) process given as: <span class="math display">\[{y_t} = {\phi _1}{y_{t - 1}} + {w_t} \text{, where } {w_t}\mathop \sim \limits^{iid} WN\left( {0,\sigma _w^2} \right)\]</span></p>
<p>This process was shown to simplify to:</p>
<p><span class="math display">\[y_t = {\phi ^t}{y_0} + \sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}}\]</span></p>
<p>In addition, we add the requirement that <span class="math inline">\(\left| \phi _1 \right| &lt; 1\)</span>. This requirement allows for the process to be stationary. If <span class="math inline">\(\phi _1 \ge 1\)</span>, the process would not converge. This way the process will be able to be written as a geometric series that converges: <span class="math display">\[\sum\limits_{k = 0}^\infty  {{r^k}}  = \frac{1}{{1 - r}},{\text{ }}\left| r \right| &lt; 1\]</span></p>
<p>Next, we demonstrate how crucial this property is:</p>
<p><span class="math display">\[\begin{aligned}
  \mathop {\lim }\limits_{t \to \infty } E\left[ {{y_t}} \right] &amp;= \mathop {\lim }\limits_{t \to \infty } E\left[ {{\phi ^t}{y_0} + \sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}} } \right] \\
   &amp;= \mathop {\lim }\limits_{t \to \infty } \underbrace {{\phi ^t}{y_0}}_{\left| \phi  \right| &lt; 1 \Rightarrow t \to \infty {\text{  = 0}}} + \sum\limits_{i = 0}^{t - 1} {\phi _1^i\underbrace {E\left[ {{w_{t - i}}} \right]}_{ = 0}}  \\
   &amp;= 0 \\
  \mathop {\lim }\limits_{t \to \infty } Var\left( {{y_t}} \right) &amp;= \mathop {\lim }\limits_{t \to \infty } Var\left( {{\phi ^t}{y_0} + \sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}} } \right) \\
   &amp;= \mathop {\lim }\limits_{t \to \infty } \underbrace {Var\left( {{\phi ^t}{y_0}} \right)}_{ = 0{\text{ since constant}}} + Var\left( {\sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}} } \right) \\
   &amp;= \mathop {\lim }\limits_{t \to \infty } \sum\limits_{i = 0}^{t - 1} {\phi _1^{2i}Var\left( {{w_{t - i}}} \right)}  \\
   &amp;= \mathop {\lim }\limits_{t \to \infty } \sigma _w^2\sum\limits_{i = 0}^{t - 1} {\phi _1^{2i}}  \\
   &amp;= \sigma _w^2 \cdot  
  \underbrace {\frac{1}{{1 - {\phi ^2}}}}_{\begin{subarray}{l} 
  {\text{Geometric Series}} 
\end{subarray}}
\end{aligned} \]</span></p>
<p>This leads us to being able to conclude the autocovariance function is: <span class="math display">\[\begin{aligned}
  Cov\left( {{y_t},{y_{t + h}}} \right) &amp;= Cov\left( {{y_t},\phi {y_{t + h - 1}} + {w_{t + h}}} \right) \\
   &amp;= Cov\left( {{y_t},\phi {y_{t + h - 1}}} \right) \\
   &amp;= Cov\left( {{y_t},{\phi ^{\left| h \right|}}{y_t}} \right) \\
   &amp;= {\phi ^{\left| h \right|}}Cov\left( {{y_t},{y_t}} \right) \\
   &amp;= {\phi ^{\left| h \right|}}Var\left( {{y_t}} \right) \\
   &amp;= {\phi ^{\left| h \right|}}\frac{{\sigma _w^2}}{{1 - \phi _1^2}} \\ 
\end{aligned} \]</span></p>
<p>Both the mean and autocovariance function do not depend on time and, thus, the AR(1) process is stationary if <span class="math inline">\(\left| \phi _1 \right| &lt; 1\)</span>.</p>
<p>If we assume that the AR(1) process is stationary, we can derive the mean and variance in another way. Without a loss of generality, we’ll assume <span class="math inline">\(y_0 = 0\)</span>.</p>
<p>Therefore:</p>
<p><span class="math display">\[\begin{aligned}
  {y_t} &amp;= {\phi _t}{y_{t - 1}} + {w_t} \\
   &amp;= {\phi _1}\left( {{\phi _1}{y_{t - 2}} + {w_{t - 1}}} \right) + {w_t} \\
   &amp;= \phi _1^2{y_{t - 2}} + {\phi _1}{w_{t - 1}} + {w_t} \\
   &amp;\vdots  \\
   &amp;= \sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}}  \\
   &amp; \\
  E\left[ {{y_t}} \right] &amp;= E\left[ {\sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}} } \right] \\
   &amp;= \sum\limits_{i = 0}^{t - 1} {\phi _1^i\underbrace {E\left[ {{w_{t - i}}} \right]}_{ = 0}}  \\
   &amp;= 0 \\
   &amp;\\
  Var\left( {{y_t}} \right) &amp;= E\left[ {{{\left( {{y_t} - E\left[ {{y_t}} \right]} \right)}^2}} \right] \\
   &amp;= E\left[ {y_t^2} \right] - {\left( {E\left[ {{y_t}} \right]} \right)^2} \\
   &amp;= E\left[ {y_t^2} \right] \\
   &amp;= E\left[ {{{\left( {{\phi _1}{y_{t - 1}} + {w_t}} \right)}^2}} \right] \\
   &amp;= E\left[ {\phi _1^2y_{t - 1}^2 + w_t^2 + 2{\phi _1}{y_t}{w_t}} \right] \\
   &amp;= \phi _1^2E\left[ {y_{t - 1}^2} \right] + \underbrace {E\left[ {w_t^2} \right]}_{ = \sigma _w^2} + 2{\phi _1}\underbrace {E\left[ {{y_t}} \right]}_{ = 0}\underbrace {E\left[ {{w_t}} \right]}_{ = 0} \\
   &amp;= \underbrace {\phi _1^2Var\left( {{y_{t - 1}}} \right) + \sigma _w^2 = \phi _1^2Var\left( {{y_t}} \right) + \sigma _w^2}_{{\text{Assume stationarity}}} \\
  Var\left( {{y_t}} \right) &amp;= \phi _1^2Var\left( {{y_t}} \right) + \sigma _w^2 \\
  Var\left( {{y_t}} \right) - \phi _1^2Var\left( {{y_t}} \right) &amp;= \sigma _w^2 \\
  Var\left( {{y_t}} \right)\left( {1 - \phi _1^2} \right) &amp;= \sigma _w^2 \\
  Var\left( {{y_t}} \right) &amp;= \frac{{\sigma _w^2}}{{1 - \phi _1^2}} \\ 
\end{aligned} \]</span></p>
</div>
</div>
<div id="estimation-of-the-mean-function" class="section level2">
<h2><span class="header-section-number">2.3</span> Estimation of the Mean Function</h2>
<p>If a time series is stationary, the mean function is constant and a possible estimator of this quantity is given by</p>
<p><span class="math display">\[\bar{X} = {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t}} }.\]</span></p>
<p>This estimator is clearly unbiased and has the following variance:</p>
<p><span class="math display">\[\begin{aligned}
  \var \left( {\bar X} \right) &amp;= \operatorname{var} \left( {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t}} } \right)  \\
   &amp;= \frac{1}{{{n^2}}}\operatorname{var} \left( {{{\left[ {\begin{array}{*{20}{c}}
  1&amp; \cdots &amp;1
\end{array}} \right]}_{1 \times n}}{{\left[ {\begin{array}{*{20}{c}}
  {{X_1}} \\
   \vdots  \\
  {{X_n}}
\end{array}} \right]}_{n \times 1}}} \right)  \\
   &amp;= \frac{1}{{{n^2}}}{\left[ {\begin{array}{*{20}{c}}
  1&amp; \cdots &amp;1
\end{array}} \right]_{1 \times n}}\left[ {\begin{array}{*{20}{c}}
  {\gamma \left( 0 \right)}&amp;{\gamma \left( 1 \right)}&amp; \cdots &amp;{\gamma \left( {n - 1} \right)} \\
  {\gamma \left( 1 \right)}&amp;{\gamma \left( 0 \right)}&amp;{}&amp; \vdots  \\
   \vdots &amp;{}&amp; \ddots &amp; \vdots  \\
  {\gamma \left( {n - 1} \right)}&amp; \cdots &amp; \cdots &amp;{\gamma \left( 0 \right)}
\end{array}} \right]{\left[ {\begin{array}{*{20}{c}}
  1 \\
   \vdots  \\
  1
\end{array}} \right]_{n \times 1}}  \\
   &amp;= \frac{1}{{{n^2}}}\left( {n\gamma \left( 0 \right) + 2\left( {n - 1} \right)\gamma \left( 1 \right) + 2\left( {n - 2} \right)\gamma \left( 2 \right) +  \cdots  + 2\gamma \left( {n - 1} \right)} \right)  \\
   &amp;= \frac{1}{n}\sum\limits_{h =  - n}^n {\left( {1 - \frac{{\left| h \right|}}{n}} \right)\gamma \left( h \right)}   \\
\end{aligned}. \]</span></p>
<p>In the white noise case, the above formula reduces to the usual <span class="math inline">\(\operatorname{var} \left( {\bar X} \right) = \operatorname{var}(X_t)/n\)</span>.</p>
<p><strong>Example:</strong> For an AR(1) we have <span class="math inline">\(\gamma(h) = \phi^h \sigma_w^2 \left(1 - \phi^2\right)^2\)</span>, therefore, we obtain (after a bit of algebra):</p>
<p><span class="math display">\[ \var \left( {\bar X} \right) = \frac{\sigma_w^2 \left( n - 2\phi - n \phi^2 + 2 \phi^{n + 1}\right)}{n^2\left(1-\phi^2\right)\left(1-\phi\right)^2}.\]</span></p>
<p>Unfortunately, deriving such an exact formula is often difficult when considering more complexe models, however, asymptotic approximations are often employed simply the calculation. For example, in our case we have</p>
<p><span class="math display">\[\mathop {\lim }\limits_{n \to \infty } \; n \var \left( {\bar X} \right) = \frac{\sigma_w^2}{\left(1-\phi\right)^2},\]</span></p>
<p>providing the following approximate formula:</p>
<p><span class="math display">\[\var \left( {\bar X} \right) \approx \frac{\sigma_w^2}{n \left(1-\phi\right)^2}.\]</span></p>
<p>Alternatively, simulation methods can also employed. The figure compares these three methods:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define sample size</span>
n =<span class="st"> </span><span class="dv">10</span>

<span class="co"># Number of Monte-Carlo replications</span>
B =<span class="st"> </span><span class="dv">5000</span>

<span class="co"># Define grid of values for phi</span>
phi =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="fl">0.95</span>, <span class="dt">to =</span> -<span class="fl">0.95</span>, <span class="dt">length.out =</span> <span class="dv">30</span>)

<span class="co"># Define result matrix</span>
result =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,B,<span class="kw">length</span>(phi))

<span class="co"># Start simulation</span>
for (i in <span class="dv">1</span>:<span class="kw">length</span>(phi)){
  <span class="co"># Define model</span>
  model =<span class="st"> </span><span class="kw">AR1</span>(<span class="dt">phi =</span> phi[i], <span class="dt">sigma2 =</span> <span class="dv">1</span>)
  
  <span class="co"># Monte-Carlo</span>
  for (j in <span class="dv">1</span>:B){
    <span class="co"># Simulate AR(1)</span>
    Xt =<span class="st"> </span><span class="kw">gen.gts</span>(model, <span class="dt">N =</span> n)
    
    <span class="co"># Estimate Xbar</span>
    result[j,i] =<span class="st"> </span><span class="kw">mean</span>(Xt)
  }
}

<span class="co"># Estimate variance of Xbar</span>
var.Xbar =<span class="st"> </span><span class="kw">apply</span>(result,<span class="dv">2</span>,var)

<span class="co"># Compute theoretical variance</span>
var.theo =<span class="st"> </span>(n -<span class="st"> </span><span class="dv">2</span>*phi -<span class="st"> </span>n*phi^<span class="dv">2</span> +<span class="st"> </span><span class="dv">2</span>*phi^(n<span class="dv">+1</span>))/(n^<span class="dv">2</span>*(<span class="dv">1</span>-phi^<span class="dv">2</span>)*(<span class="dv">1</span>-phi)^<span class="dv">2</span>)

<span class="co"># Compute (approximate) vairance</span>
var.approx =<span class="st"> </span><span class="dv">1</span>/(n*(<span class="dv">1</span>-phi)^<span class="dv">2</span>)
  
<span class="co"># Compare variance estimations</span>
<span class="kw">plot</span>(<span class="ot">NA</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(-<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim =</span> <span class="kw">range</span>(var.approx), <span class="dt">log =</span> <span class="st">&quot;y&quot;</span>, 
    <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;var(&quot;</span>, <span class="kw">bar</span>(X), <span class="st">&quot;)&quot;</span>)),
    <span class="dt">xlab=</span> <span class="kw">expression</span>(phi), <span class="dt">cex.lab =</span> <span class="fl">1.5</span>)
<span class="kw">grid</span>()
<span class="kw">lines</span>(phi,var.theo, <span class="dt">col =</span> <span class="st">&quot;deepskyblue4&quot;</span>)
<span class="kw">lines</span>(phi, var.Xbar, <span class="dt">col =</span> <span class="st">&quot;firebrick3&quot;</span>)
<span class="kw">lines</span>(phi,var.approx, <span class="dt">col =</span> <span class="st">&quot;springgreen4&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,<span class="kw">c</span>(<span class="st">&quot;Theoretical variance&quot;</span>,<span class="st">&quot;Estimated variance&quot;</span>,<span class="st">&quot;Approximate variance&quot;</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;deepskyblue4&quot;</span>,<span class="st">&quot;firebrick3&quot;</span>,<span class="st">&quot;springgreen4&quot;</span>), <span class="dt">lty =</span> <span class="dv">1</span>,
       <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>,<span class="dt">bg =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">box.col =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.2</span>)</code></pre></div>
<p><img src="tts_files/figure-html/estimXbar-1.png" width="672" /></p>
<!-- TO DO: comment grpah a little -->
</div>
<div id="sample-autocovariance-and-autocorrelation-functions" class="section level2">
<h2><span class="header-section-number">2.4</span> Sample Autocovariance and Autocorrelation Functions</h2>
<p>A natural estimator of the <strong>autocovariance function</strong> is given as:</p>
<p><span class="math display">\[\hat \gamma \left( h \right) = \frac{1}{T}\sum\limits_{t = 1}^{T - h} {\left( {{X_t} - \bar X} \right)\left( {{X_{t + h}} - \bar X} \right)} \]</span></p>
<p>leading the following “plug-in” estimator of the <strong>autocorrelation function</strong></p>
<p><span class="math display">\[\hat \rho \left( h \right) = \frac{{\hat \gamma \left( h \right)}}{{\hat \gamma \left( 0 \right)}}.\]</span></p>
<p>A graphical representation of the autocorrelation function is often the first step of any time series analysis (assuming the process to be stationary). Consider the following simulated example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load package</span>
<span class="kw">library</span>(gmwm)

<span class="co"># Simulate 100 observation from a Gaussian white noise</span>
Xt =<span class="st"> </span><span class="kw">gen.gts</span>(<span class="kw">WN</span>(<span class="dt">sigma2 =</span> <span class="dv">1</span>), <span class="dt">N =</span> <span class="dv">100</span>)

<span class="co"># Compute autocorrelation</span>
acf_Xt =<span class="st"> </span><span class="kw">ACF</span>(Xt)

<span class="co"># Plot autocorrelation</span>
<span class="kw">plot</span>(acf_Xt, <span class="dt">show.ci =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="tts_files/figure-html/basicACF-1.png" width="672" /></p>
<p>In this example, the true autocorrelation is equal to zero at any lag <span class="math inline">\(h \neq 0\)</span> but obviously the estimated autocorrelations are random variables and are not equal to their true values. It would therefore be usefull to have some knowledge about the variability of the sample autocorrelations (under some conditions) to assess whether the data comes from a completely random series or presents some significant correlation at some lags. The following result provide an asymptotic solution to this problem:</p>
<p><strong>Theorem:</strong> If <span class="math inline">\(X_t\)</span> is white noise with finite fourth moment, then <span class="math inline">\(\hat{\rho}(h)\)</span> is approximately normally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(n^{-1}\)</span> for all fixed <span class="math inline">\(h\)</span>.</p>
<!-- ADD PROOF -->
<p>Using on this result, we now have an approximate method to assess whether peaks in sample autocorrelation are significant by determining whether the observed peak lies outside the interval <span class="math inline">\(\pm 2/\sqrt{T}\)</span> (i.e. an approximate 95% confidence interval). Returning to our previous example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot autocorrelation with confidence bands </span>
<span class="kw">plot</span>(acf_Xt)</code></pre></div>
<p><img src="tts_files/figure-html/basicACF2-1.png" width="672" /></p>
<p>It can now be observed that most peaks lies within the interval <span class="math inline">\(\pm 2/\sqrt{T}\)</span> suggesting that the true data generating process is completely random (in the linear sense).</p>
<p>Unfortunately, this method is asymptotic (it relies on the central limit theorem) and there no “exact” tools that can be used in this case. In the simulation study below consider the “quality” of this result for <span class="math inline">\(h = 3\)</span> considering different sample sizes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Number of Monte Carlo replications</span>
B =<span class="st"> </span><span class="dv">10000</span>

<span class="co"># Define considered lag</span>
h =<span class="st"> </span><span class="dv">3</span>

<span class="co"># Sample size considered</span>
T =<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">30</span>,<span class="dv">300</span>)

<span class="co"># Initialisation</span>
result =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,B,<span class="kw">length</span>(T))

<span class="co"># Set seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># Start Monte Carlo</span>
for (i in <span class="dv">1</span>:B){
  for (j in <span class="dv">1</span>:<span class="kw">length</span>(T)){
    <span class="co"># Simluate process</span>
    Xt =<span class="st"> </span><span class="kw">rnorm</span>(T[j])
    
    <span class="co"># Save autocorrelation at lag h</span>
    result[i,j] =<span class="st"> </span><span class="kw">acf</span>(Xt, <span class="dt">plot =</span> <span class="ot">FALSE</span>)$acf[h<span class="dv">+1</span>]
  }
}

<span class="co"># Plot results</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">length</span>(T)))
for (i in <span class="dv">1</span>:<span class="kw">length</span>(T)){
  <span class="co"># Estimated empirical distribution</span>
  <span class="kw">hist</span>(result[,i], <span class="dt">col =</span> <span class="st">&quot;lightgrey&quot;</span>, <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;Sample size T =&quot;</span>,T[i]), <span class="dt">probability =</span> <span class="ot">TRUE</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(-<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">xlab =</span> <span class="st">&quot; &quot;</span>)
  
  <span class="co"># Asymptotic distribution</span>
  xx =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> -<span class="dv">10</span>, <span class="dt">to =</span> <span class="dv">10</span>, <span class="dt">length.out =</span> <span class="dv">10</span>^<span class="dv">3</span>)
  yy =<span class="st"> </span><span class="kw">dnorm</span>(xx,<span class="dv">0</span>,<span class="dv">1</span>/<span class="kw">sqrt</span>(T[i]))
  <span class="kw">lines</span>(xx,yy, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<p><img src="tts_files/figure-html/simulationACF-1.png" width="672" /></p>
<p>It can clearly be observed that asymptotic approximation is quite poor when <span class="math inline">\(T = 5\)</span> but as the sample size increases the approximation becomes more appropriate and is nearly perfect with <span class="math inline">\(T = 300\)</span>.</p>
<!-- TO DO: ADD MORE DETAILS ON THE SIMULATION -->
<!-- TO DO: JOINT STATIONARITY IS MISSING. 
- Add content of class notes
- Add simulated example 
- Add real bivariate example
-->
<!-- TO DO: ADD REAL DATA EXAMPLE -->
<!--
## Joint Stationarity

Two time series, say $\left(X_t \right)$ and $\left(Y_t\right)$, are said to be jointly stationary if they are each stationary, and the cross-covariance function

\[{\gamma _{XY}}\left( {t,t + h} \right) = Cov\left( {{X_t},{Y_{t + h}}} \right) = {\gamma _{XY}}\left( h \right)\]

is a function only of lag $h$.

The cross-correlation function for jointly stationary times can be expressed as:

\[{\rho _{XY}}\left( {t,t + h} \right) = \frac{{{\gamma _{XY}}\left( {t,t + h} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{Y_{t + h}}}}}} = \frac{{{\gamma _{XY}}\left( h \right)}}{{{\sigma _{{X_t}}}{\sigma _{{Y_{t + h}}}}}} = {\rho _{XY}}\left( h \right)\]

-->
</div>
<div id="robustness-issues" class="section level2">
<h2><span class="header-section-number">2.5</span> Robustness Issues</h2>
<!-- Rob I am sure you would be great to extent this section! I add a small simulation as an example -->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define sample size</span>
n =<span class="st"> </span><span class="dv">100</span>

<span class="co"># Define proportion of &quot;extreme&quot; observation</span>
alpha =<span class="st"> </span><span class="fl">0.05</span>

<span class="co"># Extreme observation are generated from N(0,sigma2.cont)</span>
sigma2.cont =<span class="st"> </span><span class="dv">10</span>

<span class="co"># Number of Monte-Carlo replications</span>
B =<span class="st"> </span><span class="dv">1000</span>

<span class="co"># Define model AR(1)</span>
phi =<span class="st"> </span><span class="fl">0.5</span>
sigma2 =<span class="st"> </span><span class="dv">1</span>
model =<span class="st"> </span><span class="kw">AR1</span>(<span class="dt">phi =</span> phi, <span class="dt">sigma2 =</span> sigma2)

<span class="co"># Initialization of result array</span>
result =<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(B,<span class="dv">2</span>,<span class="dv">20</span>))

<span class="co"># Start Monte-Carlo</span>
for (i in <span class="dv">1</span>:B){
  <span class="co"># Simulate AR(1)</span>
  Xt =<span class="st"> </span><span class="kw">gen.gts</span>(model, <span class="dt">N =</span> n)
  
  <span class="co"># Create a copy of Xt</span>
  Yt =<span class="st"> </span>Xt
  
  <span class="co"># Add a proportion alpha of extreme observations to Yt</span>
  Yt[<span class="kw">sample</span>(<span class="dv">1</span>:n,<span class="kw">round</span>(alpha*n))] =<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">round</span>(alpha*n), <span class="dv">0</span>, sigma2.cont)
  
  <span class="co"># Compute ACF of Xt and Yt</span>
  acf_Xt =<span class="st"> </span><span class="kw">ACF</span>(Xt)
  acf_Yt =<span class="st"> </span><span class="kw">ACF</span>(Yt)
  
  <span class="co"># Store ACFs</span>
  result[i,<span class="dv">1</span>,] =<span class="st"> </span>acf_Xt[<span class="dv">1</span>:<span class="dv">20</span>]
  result[i,<span class="dv">2</span>,] =<span class="st"> </span>acf_Yt[<span class="dv">1</span>:<span class="dv">20</span>]
}


<span class="co"># Compare empirical distribution of ACF based on Xt and Yt</span>

<span class="co"># Vector of lags considered (h &lt;= 20)</span>
lags =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">10</span>) +<span class="st"> </span><span class="dv">1</span>

<span class="co"># Make graph</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))

for (i in <span class="dv">1</span>:<span class="dv">4</span>){
  <span class="kw">boxplot</span>(result[,<span class="dv">1</span>,lags[i]], result[,<span class="dv">2</span>,lags[i]], <span class="dt">col =</span> <span class="st">&quot;lightgrey&quot;</span>,
          <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">&quot;Uncont.&quot;</span>,<span class="st">&quot;Cont.&quot;</span>), <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;lag: h = &quot;</span>, lags[i]-<span class="dv">1</span>),
          <span class="dt">ylab =</span> <span class="st">&quot;Sample autocorrelation&quot;</span>)
  <span class="kw">abline</span>(<span class="dt">h =</span> phi^(lags[i]-<span class="dv">1</span>), <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)
}</code></pre></div>
<p><img src="tts_files/figure-html/simulationRobust-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="basic-models.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/SMAC-Group/TTS/edit/master/02-stationarity.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
