<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Tour of Time Series Analysis with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A Tour of Time Series Analysis with R">
  <meta name="generator" content="bookdown 0.1.16 and GitBook 2.6.7">

  <meta property="og:title" content="A Tour of Time Series Analysis with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tts.smac-group.com/" />
  
  
  <meta name="github-repo" content="SMAC-Group/TTS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Tour of Time Series Analysis with R" />
  
  
  

<meta name="author" content="James Balamuta, Stéphane Guerrier, Roberto Molinari and Haotian Xu">

  
<meta name="date" content="2016-10-17">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="linear-operators-and-processes.html">
<link rel="next" href="appendixa.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { extensions: ["AMSmath.js"], 
         equationNumbers: { autoNumber: "AMS" },
         Macros: {
          notimplies: "\\nRightarrow",
          real: "\\mathbb{R}",
          integers: "\\mathbb{Z}",
          natural: "\\mathbb{N}",
          rational: "\\mathbb{Q}",
          irrational: "\\mathbb{P}",
          ind: "\\boldsymbol{1}",
          normal: "\\mathcal{N}",
          0: "\\boldsymbol{0}",
          e: ["\\mathbb{E} [#1]",1],
          I: "\\boldsymbol{\\mathbf{I}}",
          S: "\\boldsymbol{S}",
          y: "\\boldsymbol{y}",
          X: "\\boldsymbol{X}",
          C: "\\text{C}",
          btheta: "\\boldsymbol{\\theta}",
          epsilon: "\\varepsilon",
          bbeta: "\\boldsymbol{\\beta}", 
          bepsilon: "\\boldsymbol{\\varepsilon}", 
          norm: "\\mathcal{N}",
          KL: "\\text{KL}",
          AIC: "\\text{AIC}", 
          BIC: "\\text{BIC}", 
          mean: ["\\operatorname{mean}"],
          var: ["\\operatorname{var}"],
          tr: ["\\operatorname{tr}"],
          cov: ["\\operatorname{cov}"],
          corr: ["\\operatorname{corr}"],
          argmax: ["\\operatorname{argmax}"],
          argmin: ["\\operatorname{argmin}"],
          card: ["\\operatorname{card}"],
          diag: ["\\operatorname{diag}"],
          rank: ["\\operatorname{rank}"],
          length: ["\\operatorname{length}"]
    }
  }
});
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="styling/style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tour of Time Series Analysis with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i>Contributing</a></li>
<li class="chapter" data-level="" data-path="bibliographic-note.html"><a href="bibliographic-note.html"><i class="fa fa-check"></i>Bibliographic Note</a></li>
<li class="chapter" data-level="" data-path="rendering-mathematical-formulae.html"><a href="rendering-mathematical-formulae.html"><i class="fa fa-check"></i>Rendering Mathematical Formulae</a></li>
<li class="chapter" data-level="" data-path="r-code-conventions.html"><a href="r-code-conventions.html"><i class="fa fa-check"></i>R Code Conventions</a></li>
<li class="chapter" data-level="" data-path="license.html"><a href="license.html"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>1.1</b> Time Series</a></li>
<li class="chapter" data-level="1.2" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory Data Analysis for Time Series</a></li>
<li class="chapter" data-level="1.3" data-path="basicmodels.html"><a href="basicmodels.html"><i class="fa fa-check"></i><b>1.3</b> Basic Time Series Models</a><ul>
<li class="chapter" data-level="1.3.1" data-path="basicmodels.html"><a href="basicmodels.html#wn"><i class="fa fa-check"></i><b>1.3.1</b> White noise processes</a></li>
<li class="chapter" data-level="1.3.2" data-path="basicmodels.html"><a href="basicmodels.html#rw"><i class="fa fa-check"></i><b>1.3.2</b> Random Walk Processes</a></li>
<li class="chapter" data-level="1.3.3" data-path="basicmodels.html"><a href="basicmodels.html#ar1"><i class="fa fa-check"></i><b>1.3.3</b> Autoregressive Process of Order 1</a></li>
<li class="chapter" data-level="1.3.4" data-path="basicmodels.html"><a href="basicmodels.html#ma1"><i class="fa fa-check"></i><b>1.3.4</b> Moving Average Process of Order 1</a></li>
<li class="chapter" data-level="1.3.5" data-path="basicmodels.html"><a href="basicmodels.html#drift"><i class="fa fa-check"></i><b>1.3.5</b> Linear Drift</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="lts.html"><a href="lts.html"><i class="fa fa-check"></i><b>1.4</b> Composite Stochastic Processes</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="autocorrelation-and-stationarity.html"><a href="autocorrelation-and-stationarity.html"><i class="fa fa-check"></i><b>2</b> Autocorrelation and Stationarity</a><ul>
<li class="chapter" data-level="2.1" data-path="the-autocorrelation-and-autocovariance-functions.html"><a href="the-autocorrelation-and-autocovariance-functions.html"><i class="fa fa-check"></i><b>2.1</b> The Autocorrelation and Autocovariance Functions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="the-autocorrelation-and-autocovariance-functions.html"><a href="the-autocorrelation-and-autocovariance-functions.html#a-fundamental-representation"><i class="fa fa-check"></i><b>2.1.1</b> A Fundamental Representation</a></li>
<li class="chapter" data-level="2.1.2" data-path="the-autocorrelation-and-autocovariance-functions.html"><a href="the-autocorrelation-and-autocovariance-functions.html#admissible-autocorrelation-functions"><i class="fa fa-check"></i><b>2.1.2</b> Admissible Autocorrelation Functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="stationary.html"><a href="stationary.html"><i class="fa fa-check"></i><b>2.2</b> Stationarity</a><ul>
<li class="chapter" data-level="2.2.1" data-path="stationary.html"><a href="stationary.html#assessing-weak-stationarity-of-time-series-models"><i class="fa fa-check"></i><b>2.2.1</b> Assessing Weak Stationarity of Time Series Models</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="estimation-of-moments-of-stationary-processes.html"><a href="estimation-of-moments-of-stationary-processes.html"><i class="fa fa-check"></i><b>2.3</b> Estimation of Moments of Stationary Processes</a><ul>
<li class="chapter" data-level="2.3.1" data-path="estimation-of-moments-of-stationary-processes.html"><a href="estimation-of-moments-of-stationary-processes.html#estimation-of-the-mean-function"><i class="fa fa-check"></i><b>2.3.1</b> Estimation of the Mean Function</a></li>
<li class="chapter" data-level="2.3.2" data-path="estimation-of-moments-of-stationary-processes.html"><a href="estimation-of-moments-of-stationary-processes.html#sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>2.3.2</b> Sample Autocovariance and Autocorrelation Functions</a></li>
<li class="chapter" data-level="2.3.3" data-path="estimation-of-moments-of-stationary-processes.html"><a href="estimation-of-moments-of-stationary-processes.html#robustness-issues"><i class="fa fa-check"></i><b>2.3.3</b> Robustness Issues</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="joint-stationarity.html"><a href="joint-stationarity.html"><i class="fa fa-check"></i><b>2.4</b> Joint Stationarity</a><ul>
<li class="chapter" data-level="2.4.1" data-path="joint-stationarity.html"><a href="joint-stationarity.html#sample-cross-covariance-and-cross-correlation-functions"><i class="fa fa-check"></i><b>2.4.1</b> Sample Cross-Covariance and Cross-Correlation Functions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="portmanteau-test.html"><a href="portmanteau-test.html"><i class="fa fa-check"></i><b>2.5</b> Portmanteau test</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="autoregressive-moving-average-models.html"><a href="autoregressive-moving-average-models.html"><i class="fa fa-check"></i><b>3</b> Autoregressive Moving Average Models</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-operators-and-processes.html"><a href="linear-operators-and-processes.html"><i class="fa fa-check"></i><b>3.1</b> Linear Operators and Processes</a><ul>
<li class="chapter" data-level="3.1.1" data-path="linear-operators-and-processes.html"><a href="linear-operators-and-processes.html#linear-operators"><i class="fa fa-check"></i><b>3.1.1</b> Linear Operators</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-operators-and-processes.html"><a href="linear-operators-and-processes.html#linear-processes"><i class="fa fa-check"></i><b>3.1.2</b> Linear Processes</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-operators-and-processes.html"><a href="linear-operators-and-processes.html#examples-of-linear-processes"><i class="fa fa-check"></i><b>3.1.3</b> Examples of Linear Processes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="autoregressive-models.html"><a href="autoregressive-models.html"><i class="fa fa-check"></i><b>3.2</b> Autoregressive Models</a><ul>
<li class="chapter" data-level="3.2.1" data-path="autoregressive-models.html"><a href="autoregressive-models.html#properties-of-ar-models"><i class="fa fa-check"></i><b>3.2.1</b> Properties of AR models</a></li>
<li class="chapter" data-level="3.2.2" data-path="autoregressive-models.html"><a href="autoregressive-models.html#estimation-of-arp-models"><i class="fa fa-check"></i><b>3.2.2</b> Estimation of AR(<span class="math inline">\(p\)</span>) models</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixa.html"><a href="appendixa.html"><i class="fa fa-check"></i><b>A</b> Proofs</a><ul>
<li class="chapter" data-level="A.1" data-path="proof-of-theorem-1.html"><a href="proof-of-theorem-1.html"><i class="fa fa-check"></i><b>A.1</b> Proof of Theorem 1</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/SMAC-Group/TTS" target="blank">&copy; 2016 Balamuta, Guerrier, Molinari, Xu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tour of Time Series Analysis with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="autoregressive-models" class="section level2">
<h2><span class="header-section-number">3.2</span> Autoregressive Models</h2>
<p>The class of autoregressive models is based on the idea that previous values in the time series are needed to explain current values in the series. For this class of models, we assume that the <span class="math inline">\(p\)</span> previous observations are needed for this purpose and we therefore denote this class as AR(<span class="math inline">\(p\)</span>). In the previous chapter, the model we introduced was an AR(1) in which only the immediately previous observation is needed to explain the following one and therefore represents a particular model which is part of the more general class of AR(p) models.</p>

<div class="definition">
<span id="def:arp" class="definition"><strong>Definition 3.6  (Autoregressive Models of Order <span class="math inline">\(p\)</span>) </strong></span>The AR(p) models can be formally represented as follows <span class="math display">\[{X_t} = {\phi_1}{X_{t - 1}} + ... + {\phi_p}{X_{t - p}} + {W_t},\]</span> where <span class="math inline">\(\phi_p \neq 0\)</span> and <span class="math inline">\(W_t\)</span> is a (Gaussian) white noise process with variance <span class="math inline">\(\sigma^2\)</span>.
</div>
<p></p>
<p>In general, we will assume that the expectation of the process <span class="math inline">\(({X_t})\)</span>, as well as that of the following ones in this chapter, is zero. The reason for this simplification is that if <span class="math inline">\(\mathbb{E} [ X_t ] = \mu\)</span>, we can define an AR process <em>around</em> <span class="math inline">\(\mu\)</span> as follows:</p>
<p><span class="math display">\[X_t - \mu = \sum_{i = 1}^p \left(\phi_i X_{t-i} - \mu \right) + W_t,\]</span></p>
<p>which is equivalent to</p>
<p><span class="math display">\[X_t  = \mu^{\star} +  \sum_{i = 1}^p \phi_i X_{t-i}  + W_t,\]</span></p>
<p>where <span class="math inline">\(\mu^{\star} = \mu (1 - \sum_{i = 1}^p \phi_i)\)</span>. Therefore, to simplify the notation we will generally consider only zero mean processes, since adding means (as well as other deterministic trends) is easy.</p>
<p>A useful way of representing AR processes is through the backshift operator introduced in the previous section and is as follows</p>
<p><span class="math display">\[\begin{aligned}
  {X_t} &amp;= {\phi_1}{X_{t - 1}} + ... + {\phi_p}{y_{t - p}} + {w_t} \\
   &amp;= {\phi_1}B{X_t} + ... + {\phi_p}B^p{X_t} + {W_t} \\
   &amp;= ({\phi_1}B + ... + {\phi_p}B^p){X_t} + {W_t} \\ 
\end{aligned},\]</span></p>
<p>which finally yields</p>
<p><span class="math display">\[(1 - {\phi _1}B - ... - {\phi_p}B^p){X_t} = {W_t},\]</span></p>
<p>or, in abbreviated form, can be expressed as</p>
<p><span class="math display">\[\phi(B){X_t} = W_t.\]</span></p>
<p>We will see that <span class="math inline">\(\phi(B)\)</span> is important to establish the stationarity of these processes and is called the <em>autoregressive</em> operator. Moreover, this quantity is closely related to another important property of AR processes called <em>causality</em>. Before formally defining this new property, we consider the following example, which provides an intuitive illustration of its importance.</p>

<div class="example">
<p><span id="ex:ncAR1" class="example"><strong>Example 3.8  (Non-causal AR(1)) </strong></span>Consider a classical AR(1) model with <span class="math inline">\(|\phi| &gt; 1\)</span>. Such a model could be expressed as</p>
<p><span class="math display">\[X_t = \phi^{-1} X_{t+1} - \phi^{-1} W_t = \phi^{-k} X_{t+k} - \sum_{i = 1}^{k-1} \phi^{-i} W_{t+i}.\]</span></p>
<p>Since <span class="math inline">\(|\phi| &gt; 1\)</span>, we obtain</p>
<p><span class="math display">\[X_t = - \sum_{i = 1}^{\infty} \phi^{-j} W_{t-j},\]</span></p>
which is a linear process and therefore is stationary. Unfortunately, such a model is useless because we need the future to predict the future and such processes are called non-causal.
</div>
<p></p>
<div id="properties-of-ar-models" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Properties of AR models</h3>
<p>In this section we discuss the main properties of AR(<span class="math inline">\(p\)</span>) processes. We first consider the causality of these models which has already been introduced in the previous paragraphs and then discuss the autocorrelation (and partial autocorrelation) of AR(<span class="math inline">\(p\)</span>) models.</p>
<div id="causality" class="section level4">
<h4><span class="header-section-number">3.2.1.1</span> Causality</h4>
<p>In the previous section we already introduced the idea of causal time series model and therefore let us now introduce this concept in a more formal manner.</p>

<div class="definition">
<p><span id="def:causalar" class="definition"><strong>Definition 3.7  (Causality of AR models) </strong></span> An AR(p) model is said to be <em>causal</em>, if the time series <span class="math inline">\((X_t)\)</span> can be written as a one-sided linear process:</p>
<p><span class="math display">\[X_t = \sum_{j = 0}^{\infty} \psi_j W_{t-j} = \frac{1}{\phi(B)} W_t = \psi(B) W_t,\]</span></p>
where <span class="math inline">\(\phi(B) = \sum_{j = 0}^{\infty} \phi_j B^j\)</span>, <span class="math inline">\(\sum_{j=0}^{\infty}|\phi_j| &lt; \infty\)</span> and <span class="math inline">\(\phi_0 = 1\)</span>.
</div>
<p></p>
<p>As discussed earlier this condition implies that only the past values of the time series can explain the future values of it, and not vice versa. As discussed in the previous section, linear processes are (weakly) stationary processes and therefore causality directly implies (weak) stationarity. However, it might be difficult and not obvious to show the causality of AR(<span class="math inline">\(p\)</span>) processes by using the above definitions directly, thus the following property is particulary useful in practice.</p>

<div class="theorem">
<span id="thm:theocausalar" class="theorem"><strong>Theorem 3.2  (Causality of AR models) </strong></span>If an AR(p) model is causal if and only if <span class="math inline">\(\phi(z) \neq 0\)</span> for <span class="math inline">\(|z| \leq 1\)</span>. Then, the coefficients of the one-sided linear process given in <a href="autoregressive-models.html#def:causalar">3.7</a> can be obtained by solving
\begin{equation*}
    \psi(z) = \frac{1}{\sum_{j=0}^{\infty} \phi_j z^j} = \frac{1}{\phi(z)}, \mbox{ } |z| \leq 1.
\end{equation*}
</div>
<p></p>
<p>The proof of this result is omitted but can (for example) be found in Appendix B of <span class="citation">Shumway and Stoffer (<a href="#ref-shumway2010time">2010</a>)</span>. Moreover, it can be seen how there is no solution to the above equation if <span class="math inline">\(\phi(z) = 0\)</span> for <span class="math inline">\(|z| \leq 1\)</span> and therefore an AR(<span class="math inline">\(p\)</span>) is causal if and only if <span class="math inline">\(\phi(z) \neq 0\)</span> for <span class="math inline">\(|z| \leq 1\)</span>. A condition for this to be respected is for the roots of <span class="math inline">\(\phi(z) = 0\)</span> to lie <em>outside the unit circle</em>.</p>
<p>Before further discussing the properties of AR(<span class="math inline">\(p\)</span>) models we consider the following two examples which discuss the causality (and stationarity) of AR(2) models.</p>

<div class="example">
<p><span id="ex:ex1causality" class="example"><strong>Example 3.9  (Transforming an AR(2) into a linear process) </strong></span> Given the following AR(2) <span class="math inline">\(X_t = 1.3X_{t-1} - 0.4X_{t-2} + W_t\)</span> one could wonder if this process can be written as a one-sided linear process as in Definition <a href="autoregressive-models.html#def:causalar">3.7</a>. This can be done using the following approach:</p>
<p><em>Step 1:</em> The AR operator of this model can be expressed as:</p>
<p><span class="math display">\[\phi(z) = 1 - 1.3z + 0.4z^2 = (1 - 0.5z)(1 - 0.8z) ,\]</span></p>
<p>and has roots <span class="math inline">\(2 &gt; 1\)</span> and <span class="math inline">\(1.25 &gt; 1\)</span>. Thus, we should be able to covert it into linear process.</p>
<p><em>Step 2:</em> From Theorem (<span class="citation">(<span class="citeproc-not-found" data-reference-id="theocausalar"><strong>???</strong></span>)</span>) we know that if AR process has all its roots lie outside the unit circle, we can write <span class="math inline">\(X_t = \phi^{-1}(B)W_t\)</span> and “inverse” the autoregressive operator <span class="math inline">\(\phi(B)\)</span> as follows:</p>
<p><span class="math display">\[\phi^{-1}(z) = \frac{1}{(1 - 0.5z)(1 - 0.8z)} = \frac{c_1}{(1 - 0.5z)} + \frac{c_2}{(1 - 0.8z)} = \frac{c_2(1 - 0.5z) + c_1(1 - 0.8z)}{(1 - 0.5z)(1 - 0.8z)} ,\]</span></p>
<p>since we can think of the above equation is valid for any <span class="math inline">\(z\)</span>, we will solve the following system of equations to get <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>,</p>
<p><span class="math display">\[\begin{cases}
        c_1 + c_2 &amp;= 1\\
        -0.5 c_2 - 0.8 c_1 &amp;= 0
\end{cases}
\implies 
\begin{cases}
        c_1 &amp;= -\frac{5}{3}\\
        c_2 &amp;= \frac{8}{3},
\end{cases}
\]</span></p>
<p>and, we obtain</p>
<p><span class="math display">\[\phi^{-1}(z) = \frac{-5}{3(1 - 0.5z)} + \frac{8}{3(1 - 0.8z)} .\]</span></p>
<p><em>Step 3:</em> Using Geometric series, i.e. <span class="math inline">\(a \sum_{k = 0}^{\infty}r^k = \frac{a}{1-r}, \mbox{ if } |r| &lt; 1\)</span>, we have</p>
<p><span class="math display">\[\begin{cases}
      \frac{-5}{3(1 - 0.5z)} &amp;= \frac{-5}{3} \sum_{j = 0}^{\infty}0.5^jz^j , \mbox{ if } |z| &lt; 2\\
      \frac{8}{3(1 - 0.8z)} &amp;= \frac{8}{3} \sum_{j = 0}^{\infty}0.8^jz^j , \mbox{ if } |z| &lt; 1.25.
\end{cases}\]</span></p>
<p>This allows to expressed <span class="math inline">\(\phi^{-1}(z)\)</span> as</p>
<p><span class="math display">\[\phi^{-1}(z) = \sum_{j = 0}^{\infty} \left[ \frac{-5}{3} (0.5)^j + \frac{8}{3} (0.8)^j \right] z^j, \mbox{ if } |z| &lt; 1.25 .\]</span></p>
<em>Step 4:</em> Finally, we obtain <span class="math display">\[X_t = \phi^{-1}(B)W_t = \sum_{j = 0}^{\infty} \left[ \frac{-5}{3} (0.5)^j + \frac{8}{3} (0.8)^j \right] B^j W_t = \sum_{j = 0}^{\infty} \left[ \frac{-5}{3} (0.5)^j + \frac{8}{3} (0.8)^j \right] W_{t-j},
\]</span> which verifies that the AR(2) is causal (and stationary).
</div>
<p></p>

<div class="example">
<p><span id="ex:ex2causality" class="example"><strong>Example 3.10  (Causal conditions for an AR(2) processes) </strong></span>We already know that an AR(1) is causal with the simple condition <span class="math inline">\(|\phi_1| &lt; 1\)</span>. It could seem natural to believe that an AR(2) should be causal (implies stationary) with the conditon: <span class="math inline">\(|\phi_i| &lt; 1, \, i = 1,2\)</span>, however, this is not the case. Indeed, an AR(2) can be expressed as</p>
<p><span class="math display">\[X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + W_t = \phi_1 B X_t + \phi_2 B^2 X_t + W_t,\]</span></p>
<p>corresponding to the following autoregressive operator:</p>
<p><span class="math display">\[\phi(z) = 1 - \phi_1 z - \phi_2 z^2.\]</span></p>
<p>Therefore, the process is causal when the roots of <span class="math inline">\(\phi(z)\)</span> lies outside of the unit circle. Letting <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> denote those roots, we impose the following constraints to ensure the causality of the model:</p>
<p><span class="math display">\[\begin{aligned}
|z_1| &amp;&gt; 1, \;\;\;\; \text{where} \;\; &amp;z_1 = \frac{\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{-2 \phi_2},\\
|z_2| &amp;&gt; 1, \;\;\;\; \text{where} \;\; &amp;z_2 = \frac{\phi_1 - \sqrt{\phi_1^2 + 4\phi_2}}{-2 \phi_2},
\end{aligned}\]</span> note that <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> can be complex values.</p>
<p>Thus we can represent <span class="math inline">\(\phi_1\)</span> and <span class="math inline">\(\phi_2\)</span> by <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span>, <span class="math display">\[\begin{aligned}
\phi_1 = (z_1^{-1} + z_2^{-1}),\\
\phi_2 = -(z_1 z_2)^{-1}.
\end{aligned}\]</span></p>
<p>Moreover we have the following equivalent condition for causality:</p>
<p><span class="math display">\[\begin{cases}
        |z_1| &amp;&gt; 1\\
        |z_2| &amp;&gt; 1,
\end{cases}\]</span> if and only if <span class="math display">\[\begin{cases}
        \phi_1 + \phi_2 &amp;&lt; 1\\
        \phi_2 - \phi_1 &amp;&lt; 1\\
        |\phi_2| &amp;&lt; 1.
\end{cases}\]</span></p>
<p>We can show “<em>if</em>”</p>
<p><span class="math inline">\(\phi_1 + \phi_2 = \frac{1}{z_1} + \frac{1}{z_2} - \frac{1}{z_1z_2} = \frac{1}{z_1}\left( 1 - \frac{1}{z_2} \right) + \frac{1}{z_2} &lt; 1 - \frac{1}{z_2} + \frac{1}{z_2} = 1\)</span>, (since <span class="math inline">\(\left( 1 - \frac{1}{z_2} \right) &gt; 0\)</span>)</p>
<p><span class="math inline">\(\phi_2 - \phi_1 = - \frac{1}{z_1z_2} - \frac{1}{z_1} - \frac{1}{z_2} = - \frac{1}{z_1}\left( \frac{1}{z_2} + 1 \right) - \frac{1}{z_2} &lt; \frac{1}{z_2} + 1 - \frac{1}{z_2} = 1\)</span>, (since <span class="math inline">\(\left( \frac{1}{z_2} + 1 \right) &gt; 0\)</span>)</p>
<p><span class="math inline">\(|\phi_2| = \frac{1}{|z_1||z_2|} &lt; 1\)</span>.</p>
<p>We can also show “<em>only if</em>”</p>
<p>Since <span class="math inline">\(z_1 = \frac{\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{-2 \phi_2}\)</span> and <span class="math inline">\(\phi_2-1 &lt; \phi_1 &lt; 1-\phi_2\)</span>, then <span class="math inline">\(z_1^2 = \frac{\left(\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}\right)^2}{4 \phi_2^2} &lt; \frac{\left((1-\phi_2) + \sqrt{(1-\phi_2)^2 + 4\phi_2}\right)^2}{4 \phi_2^2} = \frac{4}{4 \phi_2^2} \leq 1\)</span>.</p>
Since <span class="math inline">\(z_2 = \frac{\phi_1 - \sqrt{\phi_1^2 + 4\phi_2}}{-2 \phi_2}\)</span> and <span class="math inline">\(\phi_2-1 &lt; \phi_1 &lt; 1-\phi_2\)</span>, then <span class="math inline">\(z_2^2 = \frac{\left(\phi_1 - \sqrt{\phi_1^2 + 4\phi_2}\right)^2}{4 \phi_2^2} &lt; \frac{\left((\phi_2-1) + \sqrt{(\phi_2-1)^2 + 4\phi_2}\right)^2}{4 \phi_2^2} = \frac{4 \phi_2^2}{4 \phi_2^2} = 1\)</span>.
</div>
<p></p>
<p>Finally, the causal region of an AR(2) is depicted on the figure below.</p>
<p><img src="tts_files/figure-html/causalAR2-1.png" width="672" /></p>
</div>
<div id="autocorrelation" class="section level4">
<h4><span class="header-section-number">3.2.1.2</span> Autocorrelation</h4>
<p>In this section we discuss the autocorrelation of (causal) AR(<span class="math inline">\(p\)</span>) processes. Before considering the general case of an AR(<span class="math inline">\(p\)</span>), we revisit Example <a href="autoregressive-models.html#ex:ex1causality">3.9</a> and derive the ACF of the AR(2) model presented in this example.</p>

<div class="example">
<p><span id="ex:ar2acf" class="example"><strong>Example 3.11  (Autocorrelation of an AR(2)) </strong></span>Considering the same model as in Example <a href="autoregressive-models.html#ex:ex1causality">3.9</a>, i.e. <span class="math inline">\(X_t = 1.3X_{t-1} - 0.4X_{t-2} + W_t\)</span>, we can derive the ACF using the following steps:</p>
<p><em>Step 1:</em> Find the homogeneous difference equation with respect to the ACF <span class="math inline">\(\rho(h)\)</span>, which in this case is given by: <span class="math display">\[\rho(h) - 1.3\rho(h-1) + 0.4\rho(h-2) = 0, \mbox{   } h = 1,2,...\]</span> and the initial conditions are <span class="math inline">\(\rho(0) = 1\)</span> and <span class="math inline">\(\rho(-1) = \frac{13}{14}\)</span>. Note that the above equation is an homogenous difference equation of order 2.</p>
<p><em>Step 2:</em> Using the results of Example <a href="autoregressive-models.html#ex:ex1causality">3.9</a>, we have: <span class="math display">\[\phi(z) = 1 - 1.3z + 0.4z^2 = (1 - 0.5z)(1 - 0.8z) ,\]</span> and the roots of this equation are given by <span class="math inline">\(z_1 = 2 &gt; 1\)</span> and <span class="math inline">\(z_2 = 1.25 &gt; 1\)</span>. Moreover, since <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> are real and distinct, we obtain (since it corresponds to the solution of an homogenous difference equation of order 2):</p>
<p><span class="math display">\[\rho(h) = c_1 z_1^{-h} + c_2 z_2^{-h}.\]</span></p>
<em>Step 3:</em> Solve <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> based on two initial conditions found in Step 1, i.e. <span class="math display">\[\begin{cases}
      \rho(0) = c_1 + c_2 = 1\\
      \rho(-1) = 2c_1 + 1.25c_2 = \frac{13}{14},
\end{cases},\]</span> implying that <span class="math inline">\(c_1 = -\frac{3}{7}\)</span> and <span class="math inline">\(c_2 = \frac{10}{7}\)</span>. The ACF for this model is therefore given by <span class="math display">\[\rho(h) = -\frac{3}{7}2^{-h} + \frac{10}{7}\left(\frac{5}{4}\right)^{-h} .\]</span>
</div>
<p></p>
<p>The graph depicts the ACF of this process:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(exts)
<span class="kw">autoplot</span>(<span class="kw">theo_acf</span>(<span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">1.3</span>, -<span class="fl">0.4</span>))))</code></pre></div>
<p><img src="tts_files/figure-html/ACFAR2_cont-1.png" width="576" /></p>
<p>The method used in the previous is only applicable for AR(2) with roots that are distinct and real, which is true when <span class="math inline">\(\phi_2 &gt; - \phi^1/4\)</span>. In the case where <span class="math inline">\(\phi_2 = - \phi^1/4\)</span>, the autoregressive operator has single root and <span class="math inline">\(\rho(h)\)</span> can be obtain by determining (using inital conditions) the constants <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> of the following expression:</p>
<p><span class="math display">\[\rho(h) = z_1^{-h} \left(c_1 + c_2 h\right).\]</span> When <span class="math inline">\(\phi_2 &gt; - \phi^1/4\)</span> the roots are complexe conjugate pairs and solution is given by:</p>
<p><span class="math display">\[\rho(h) = c_1 |z_1|^{-h} \cos \left(h \theta + c_2\right), \]</span> where the constants depends on inital conditions, while <span class="math inline">\(\theta = \text{arg}(z_1)\)</span>.</p>
<p>Therefore, we have that <span class="math inline">\(\rho(h) \to 0\)</span> exponential fast as <span class="math inline">\(h \to \infty\)</span> and when <span class="math inline">\(\phi_2 &gt; - \phi^1/4\)</span> <span class="math inline">\(\rho(h)\)</span> goes to zero in an sinusoidal fashion. This behaviour is illustrated in the next example.</p>

<div class="example">
<p><span id="ex:ar2acfExample" class="example"><strong>Example 3.12  (Autocorrelation AR(2) Processes) </strong></span>Consider the following models:</p>
<p><span class="math display">\[\begin{aligned}
  \text{Model 1}: \;\;\; X_t &amp;= X_{t-1} - 0.25 X_{t-2} + W_t  \\
  \text{Model 2}: \;\;\; X_t &amp;= 0.5 X_{t-1} + 0.25 X_{t-2} + W_t  \\
  \text{Model 3}: \;\;\; X_t &amp;= -1.5 X_{t-1} - 0.75 X_{t-2} + W_t.
\end{aligned} \]</span></p>
It is easy to verify the first one has real distrinct roots, the second a unique real root while the latter has complexe roots. This is illustrated in the figure which depicts the causal region of an AR(2) that has been separated between models with real and complexe roots.
</div>
<p></p>
<p>Next we present an example to show how to derive the ACF for general causal AR(p) model. It is much more complicated than what we did for AR(2).</p>
<p><img src="tts_files/figure-html/causalAR2_part2-1.png" width="672" /></p>
<p>The ACF of these models are represented on the figure below. As expected the three models corresponds to an ACF that dempends expoentially fast but only Model 3 exibits a sinusoidal features.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gridExtra)

<span class="co"># Define models</span>
m1 =<span class="st"> </span><span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="dv">1</span>, -<span class="fl">0.25</span>))
m2 =<span class="st"> </span><span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">0.5</span>,<span class="fl">0.25</span>))
m3 =<span class="st"> </span><span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(-<span class="fl">1.5</span>, -<span class="fl">0.75</span>))

<span class="co"># Theoretical ACF</span>
acf1 =<span class="st"> </span><span class="kw">theo_acf</span>(m1)
acf2 =<span class="st"> </span><span class="kw">theo_acf</span>(m2)
acf3 =<span class="st"> </span><span class="kw">theo_acf</span>(m3)

<span class="co"># Plot ACFs</span>
a1 =<span class="st"> </span><span class="kw">autoplot</span>(acf1)
a2 =<span class="st"> </span><span class="kw">autoplot</span>(acf2)
a3 =<span class="st"> </span><span class="kw">autoplot</span>(acf3)
<span class="kw">grid.arrange</span>(a1, a2, a3, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="tts_files/figure-html/ACFAR2eg-1.png" width="864" /></p>
<p>Next, we consider the ACF for a general causal AR(<span class="math inline">\(p\)</span>) model. Unfortunatelty, this is far more complicated than our previous example.</p>

<div class="example">
<p><span id="ex:arpacf" class="example"><strong>Example 3.13  (Autocorrelation of an AR(<span class="math inline">\(p\)</span>)) </strong></span> Recall that the AR(<span class="math inline">\(p\)</span>) models can be formally represented as follows <span class="math display">\[{X_t} = {\phi_1}{X_{t - 1}} + ... + {\phi_p}{X_{t - p}} + {W_t},\]</span> where <span class="math inline">\(W_t\)</span> is a (Gaussian) white noise process with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>There are two ways to derive the ACF for general AR(<span class="math inline">\(p\)</span>) models. For the first one, since we assume our AR(<span class="math inline">\(p\)</span>) model is causal, we can write it as a one-sided linear process: <span class="math inline">\(X_t = \sum_{j = 0}^{\infty} \psi_j W_{t-j}\)</span>, then the autocovariance function <span class="math inline">\(\gamma(h) = cov(X_{t+h},X_t) = \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+h}\)</span>, for <span class="math inline">\(h \geq 0\)</span>. The difficulty of this method is to solve the <span class="math inline">\(\psi\)</span>-weights. Treating AR(p) as a special case of ARMA(p,q), we can let the MA polynomials <span class="math inline">\(\theta(z) = 1\)</span> and solve the <span class="math inline">\(\psi\)</span>-weights by matching the coefficients in <span class="math inline">\(\phi(z)\psi(z) = \theta(z)\)</span>.</p>
<p>For the second method, we can also use the procedure we used for AR(2). That is finding a homogeneous difference equation with respect to ACF <span class="math inline">\(\rho(h)\)</span>, and solve it directly. To do so we need following steps.</p>
<p><em>Step 1</em>: Find the homogeneous difference equation with repsect to the ACF <span class="math inline">\(\rho(h)\)</span>.</p>
<p>Firstly verify whether our model is causal. If it is then we have, <span class="math display">\[\rho(h) - \phi_1\rho(h-1) - \cdots - \phi_p\rho(h-p) = 0, \mbox{ } h \geq p. \]</span></p>
<p><em>Step 2</em>: Solve the roots of the associated AR polynomials.</p>
<p>The polynomials can be written as: <span class="math display">\[\phi(z) = 1 - \phi_1z - \cdots - \phi_pz^p .\]</span></p>
<p>Suppose the polynomials have <span class="math inline">\(r\)</span> distinct roots, and let <span class="math inline">\(m_j\)</span> denoted the number of replicates of <span class="math inline">\(z_j\)</span>, such that <span class="math inline">\(m_1 +m_2 + \cdots +m_r = p\)</span>.</p>
<p>Then the general solution of the homogeneous difference equation is <span class="math display">\[\rho(h) = z_1^{-h}P_1(h) + z_2^{-h}P_2(h) + \cdots + z_r^{-h}P_r(h), \mbox{ } h \geq p,\]</span> where <span class="math inline">\(P_j(h)\)</span> is the polynomial in <span class="math inline">\(h\)</span> of degree <span class="math inline">\(m_j - 1\)</span>.</p>
<p><em>Step 3</em>: Solve every <span class="math inline">\(P_j(h)\)</span> based on <span class="math inline">\(p\)</span> given initial conditions on <span class="math inline">\(\rho(h)\)</span>.</p>
<p><strong>Remark:</strong> Since the AR(<span class="math inline">\(p\)</span>) model is causal, the roots we obtained in step 2 should be outside of the unit circle (i.e. <span class="math inline">\(|z_i| &gt; 1\)</span>, for <span class="math inline">\(i = 1, \cdots, r\)</span>). Then the absolute value of the general solution <span class="math display">\[|\rho(h)| = \left|\frac{P_1(h)}{z_1^{h}} + \frac{P_2(h)}{z_2^{h}} + \cdots + \frac{P_r(h)}{z_r^{h}}\right| \leq \left|\frac{P_1(h)}{z_1^{h}}\right| + \left| \frac{P_2(h)}{z_2^{h}}\right| + \dots + \left|\frac{P_r(h)}{z_r^{h}}\right| \leq \frac{r \left|P_r(h)\right|}{\min_{j = 1, \dots, r} \left|z_j \right|^{h}},\]</span></p>
from the right hand side of the last inequality, we can find the rate of convergence would be dominated by <span class="math inline">\(\frac{1}{\min_{j = 1, \dots, r} \left|z_j \right|^{h}}\)</span>. Thus the ACF <span class="math inline">\(\rho(h)\)</span> will goes to zero exponentially as <span class="math inline">\(h \to \infty\)</span>.
</div>
<p></p>
<p>EXAMPLE MISSING HERE + DISCUSSION ON LIMITS OF ACF</p>
</div>
<div id="partial-autocorrelation-of-ar-models" class="section level4">
<h4><span class="header-section-number">3.2.1.3</span> Partial autocorrelation of AR models</h4>

<div class="definition">
<span id="def:pacf" class="definition"><strong>Definition 3.8  (Partial autocorrelation) </strong></span>For a stationary process, <span class="math inline">\(X_t\)</span>, the Partial AutoCorrelation Function (PACF) can be denoted as <span class="math inline">\(\phi_{hh}\)</span>, for <span class="math inline">\(h = 1, 2, \dots,\)</span> which are <span class="math display">\[\phi_{11} = \corr(X_{t+1}, X_t) = \rho(1),\]</span> and <span class="math display">\[\phi_{hh} = \corr(X_{t+h} - \hat{X}_{t+h}, X_t - \hat{X}_t), \mbox{ } h \geq 2.\]</span>
</div>
<p></p>
<p><strong>Remark</strong> From the above defination we can think of the partial correlation <span class="math inline">\(\phi_{hh}\)</span> as the correlation between the residual of <span class="math inline">\(X_{t+h}\)</span> after removing its best linear predictor on the vector space spanned by <span class="math inline">\(\{ X_{t+1}, \dots, X_{t+h-1} \}\)</span> and the residual of <span class="math inline">\(X_t\)</span> after reomving its best linear predictor on the same vector space. Similar to the linear regression, after projecting the residuals should be independ with the vector space spanned by <span class="math inline">\(\{ X_{t+1}, \dots, X_{t+h-1} \}\)</span>.</p>
<p>We will discuss about the best linear predictor in the section Forecasing, here we just mention how to obtain the best linear predictor. Given a time serie <span class="math inline">\(X_1, X_2, \dots, X_t\)</span> with zero mean, the best linear predictor of <span class="math inline">\(X_{t+h}\)</span> can be written as <span class="math inline">\(\hat{X}_{t+h} = \sum_{j=1}^t \alpha_j X_j\)</span> such than it satisfies the prediction equations: <span class="math display">\[ \mathbb{E}(X_{t+h} - \hat{X}_{t+h}) = 0, \]</span> and <span class="math display">\[ \mathbb{E} [(X_{t+h} - \hat{X}_{t+h})X_j ] = 0, \mbox{ for } i = 1, \dots, t.\]</span> According to the projection theorem, we can show that satisfying the prediction equations is equivalent to minimizing the mean square error <span class="math inline">\(\mathbb{E}(X_{t+h} - \hat{X}_{t+h})\)</span>. Thus we have two equivalent to obtain the best linear predictor.</p>

<div class="example">
<p><span id="ex:pacfar1" class="example"><strong>Example 3.14  (PACF of AR(1)) </strong></span>Consider a casual AR(1) model <span class="math inline">\({X_t} = \phi {X_{t - 1}} + {W_t},\)</span> We have, <span class="math display">\[\phi_{11} = \corr(X_{t+1}, X_t) = \rho(1) = \phi,\]</span> and <span class="math display">\[\phi_{22} = \corr(X_{t+2} - \hat{X}_{t+2}, X_t - \hat{X}_t),\]</span> Next, we find <span class="math inline">\(\hat{X}_t\)</span> and <span class="math inline">\(\hat{X}_{t+2}\)</span>. Since <span class="math inline">\(X_{t+1}\)</span> is the only random between <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+2}\)</span>, <span class="math inline">\(\hat{X}_t\)</span> and <span class="math inline">\(\hat{X}_{t+2}\)</span> are the best linear predictors on the vector space spaned by <span class="math inline">\(X_t\)</span>, we can obtain them by minimizing the MSE. <span class="math display">\[\mathbb{E}(X_{t+2} - \hat{X}_{t+2})^2 = \mathbb{E}(X_{t+2} - \beta_1 X_{t+1})^2 = \gamma(0) - 2\beta_1 \gamma(1) + \beta_1^2 \gamma(0),\]</span> Then by minimizing the MSE, we have <span class="math inline">\(\beta_1 = \frac{\gamma(1)}{\gamma(0)} = \phi\)</span>.</p>
<p>Similarily, by minimizing <span class="math display">\[\mathbb{E}(X_{t} - \hat{X}_{t})^2 = \mathbb{E}(X_{t} - \beta_2 X_{t+1})^2 = \gamma(0) - 2\beta_2 \gamma(1) + \beta_2^2 \gamma(0),\]</span> we have <span class="math inline">\(\beta_2 = \frac{\gamma(1)}{\gamma(0)} = \phi\)</span>.</p>
<p>Or equivalently we can use the prediction equations. Thus we have <span class="math display">\[\mathbb{E}[(X_{t+2} - \hat{X}_{t+2})X_{t+1}] = \mathbb{E}[(X_{t+2}X_{t+1} - \beta_1 X_{t+1}^2)] = \gamma(1) - \beta_1 \gamma(0) = 0,\]</span> and <span class="math display">\[\mathbb{E}[(X_{t} - \hat{X}_{t})X_{t+1}] = \mathbb{E}[(X_{t}X_{t+1} - \beta_2 X_{t+1}^2)] = \gamma(1) - \beta_2 \gamma(0) = 0,\]</span> Thus we can get the same solutions.</p>
Therefore, <span class="math display">\[\phi_{22} = \corr(X_{t+2} - \phi X_{t+1}, X_t - \phi X_{t+1}) = \corr(W_{t+2}, X_t - \phi X_{t+1}) = 0,\]</span> note that the last equation is based on casuality.
</div>
<p></p>

<div class="example">
<p><span id="ex:pacfarp" class="example"><strong>Example 3.15  (PACF of AR(p)) </strong></span>In this example we would like to show that the PACF characterize the order of AR(p) models. That is when <span class="math inline">\(h &gt; p\)</span>, the PACF <span class="math inline">\(\phi_{hh} = 0\)</span>. Suppose a causal AR(p) model, <span class="math inline">\(X_{t+h} = \sum_{j=1}^p \phi_j X_{t+h-j} + W_{t+h}\)</span>, we want to calculate <span class="math inline">\(\phi_{hh}\)</span>.</p>
The best linear predictor of <span class="math inline">\(X_{t+h}\)</span> is <span class="math display">\[\hat{X}_{t+h} = \mathbb{E}\left[X_{t+h}|X_t, \dots, X_{t+h-1}\right] = \mathbb{E}\left[\sum_{j=1}^p \phi_j X_{t+h-j} + W_{t+h}|X_t, \dots, X_{t+h-1}\right] = \sum_{j=1}^p \mathbb{E}\left[\phi_j X_{t+h-j}|X_t, \dots, X_{t+h-1}\right] = \sum_{j=1}^p \phi_j X_{t+h-j}.\]</span> Thus when <span class="math inline">\(h &gt; p\)</span>, <span class="math display">\[ \phi_{hh} = corr(X_{t+h} - \hat{X}_{t+h}, X_t - \hat{X}_t) = corr(W_{t+h}, X_t - \hat{X}_t) = 0. \]</span>
</div>
<p></p>
</div>
</div>
<div id="estimation-of-arp-models" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Estimation of AR(<span class="math inline">\(p\)</span>) models</h3>
<p>Given the above defined properties of the AR(<span class="math inline">\(p\)</span>) models, we will now discuss how these models can be estimated, more specifically how the <span class="math inline">\(p+1\)</span> parameters can be obtained from an observed time series. Indeed, a reliable estimation of these models is necessary in order to interpret and describe different natural phenomena and/or forecast possible future values of the time series.</p>
A first approach builds upon the earlier definition of the AR(p) as being a linear process. Recall that
\begin{equation}
    X_t = \sum_{j = 1}^{p} \phi_j X_{t-j} + W_t
\end{equation}
which delivers the following autocovariance function
\begin{equation}
    \gamma(h) = cov(X_{t+h}, X_t) = cov(\sum_{j = 1}^{p} \phi_j X_{t-j} + W_{t+h}, X_t) = \sum_{j = 1}^{p} \phi_j \gamma(h-j), \mbox{ } h \geq 0.
\end{equation}
Rearranging the above expressions we obtain the following general equations
\begin{equation}
    \gamma(h) - \sum_{j = 1}^{p} \phi_j \gamma(h-j) = 0, \mbox{ } h \geq 1
\end{equation}
and, recalling that <span class="math inline">\(\gamma(h) = \gamma(-h)\)</span>,
\begin{equation}
    \gamma(0) - \sum_{j = 1}^{p} \phi_j \gamma(j) = \sigma_w^2.
\end{equation}
<p>We can now define the Yule-Walker equations.</p>

<div class="definition">
<span id="def:yweqs" class="definition"><strong>Definition 3.9  (Yule-Walker Equations) </strong></span>The Yule-Walker equations are given by
\begin{equation}
    \gamma(h) = \phi_1 \gamma(h-1) + ... + \phi_p \gamma(h-p), \mbox{ } h = 1,...,p
\end{equation}
and
\begin{equation}
    \sigma_w^2 = \gamma(0) - \phi_1 \gamma(1) - ... - \phi_p \gamma(p).
\end{equation}
which in matrix notation can be defined as follows
\begin{equation}
    \Gamma_p \mathbf{\phi} = \mathbf{\gamma}_p \text{and} \sigma_w^2 = \gamma(0) - \mathbf{\phi}&#39;\mathbf{\gamma}_p
\end{equation}
where <span class="math inline">\(\Gamma_p\)</span> is the <span class="math inline">\(p\times p\)</span> matrix containing the autocovariances <span class="math inline">\(\gamma(k-j), j,k = 1, ...,p\)</span> while <span class="math inline">\(\mathbf{\phi} = (\phi_1,...,\phi_p)&#39;\)</span> and <span class="math inline">\(\mathbf{\gamma}_p = (\gamma(1),...,\gamma(p))&#39;\)</span> are <span class="math inline">\(p\times 1\)</span> vectors.
</div>
<p></p>
Considering the Yule-Walker equations, it is possible to use a method of moments approach and simply replace the theoretical quantities given in the previous definition with their empirical (estimated) counterparts that we saw in the previous chapter. This gives us the following Yule-Walker estimators
\begin{equation}
    \hat{\mathbf{\phi}} = \hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p \text{and} \hat{\sigma}_w^2 = \hat{\gamma}(0) - \hat{\mathbf{\gamma}}_p&#39;\hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p
\end{equation}
<p>These estimators have the following asymptotic properties.</p>
<p><strong>Property: Consistency and Asymptotic Normality of Yule-Walker estimators</strong> The Yule-Walker estimators for a causal AR(p) model have the following asymptotic properties:</p>
\begin{equation*}
\sqrt{T}(\hat{\mathbf{\phi}}- \mathbf{\phi}) \xrightarrow{\mathcal{D}} \mathcal{N}(\mathbf{0},\sigma_w^2\Gamma_p^{-1}) \text{and} \hat{\sigma}_w^2 \xrightarrow{\mathcal{P}} \sigma_w^2 .
\end{equation*}
Therefore the Yule-Walker estimators have an asymptotically normal distribution and the estimator of the innovation variance is consistent. Moreover, these estimators are also optimal for AR(p) models, meaning that they are also efficient. However, there exists another method which allows to achieve this efficiency also for general ARMA models and this is the maximum likelihood method. Considering an AR(1) model as an example, and assuming without loss of generality that the expectation is zero, we have the following representation of the AR(1) model
\begin{equation*}
X_t = \phi X_{t-1} + W_t
\end{equation*}
where <span class="math inline">\(|\phi|&lt;1\)</span> and <span class="math inline">\(W_t \overset{iid}{\sim} \mathcal{N}(0,\sigma_w^2)\)</span>. Supposing we have observations issued from this model <span class="math inline">\((x_t)_{t=1,...,T}\)</span>, then the likelihood function for this setting is given by
\begin{equation*}
L(\phi,\sigma_w^2) = f(\phi,\sigma_w^2|x_1,...,x_T)
\end{equation*}
which, for an AR(1) model, can be rewritten as follows
\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1)f(x_2|x_1)\cdot \cdot \cdot f(x_T|x_{T-1}).
\end{equation*}
If we define <span class="math inline">\(\Omega_t^p\)</span> as the information contained in the previous <span class="math inline">\(p\)</span> observations to time <span class="math inline">\(t\)</span>, the above can be generalized for an AR(p) model as follows
\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1,...,x_p)f(x_{p+1}|\Omega_{p+1}^p)\cdot \cdot \cdot f(x_T|\Omega_{T-1}^p)
\end{equation*}
where <span class="math inline">\(f(x_1,...,x_p)\)</span> is the joint probability distribution of the first <span class="math inline">\(p\)</span> observations. A discussion on how to find <span class="math inline">\(f(x_1,...,x_p)\)</span> will be presented in the following paragraphs based on the approach to find <span class="math inline">\(f(x_1)\)</span> in the AR(1) likelihood. Going back to the latter, we know that <span class="math inline">\(x_t|x_{t-1} \sim \mathcal{N}(\phi x_{t-1},\sigma_w^2)\)</span> and therefore we have that
\begin{equation*}
f(x_t|x_{t-1}) = f_w(x_t - \phi x_{t-1})
\end{equation*}
where <span class="math inline">\(f_w(\cdot)\)</span> is the distribution of <span class="math inline">\(w_t\)</span>. This rearranges the likelihood function as follows
\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1)\prod_{t=2}^T f_w(x_t - \phi x_{t-1})
\end{equation*}
where <span class="math inline">\(f(x_1)\)</span> can be found through the causal representation
\begin{equation*}
x_1 = \sum_{j=0}^{\infty} \phi^j w_{1-j} 
\end{equation*}
which implies that <span class="math inline">\(x_1\)</span> follows a normal distribution with zero expectation and a variance given by <span class="math inline">\(\frac{\sigma_w^2}{(1-\phi^2)}\)</span>. Based on this, the likelihood function of an AR(1) finally becomes
\begin{equation*}
L(\phi,\sigma_w^2) = (2\pi \sigma_w^2)^{-\frac{n}{2}} (1 - \phi)^2 \exp \left(-\frac{S(\phi)}{2 \sigma_w^2}\right)
\end{equation*}
with <span class="math inline">\(S(\phi) = (1-\phi)^2 x_1^2 + \sum_{t=2}^T (x_t -\phi x_{t-1})^2\)</span>. Once the derivative of the logarithm of the likelihood is taken, the minimization of the negative of this function is usually done numerically. However, if we condition on the initial values, the AR(p) models are linear and, for example, we can then define the conditional likelihood of an AR(1) as
\begin{equation*}
L(\phi,\sigma_w^2|x_1) = (2\pi \sigma_w^2)^{-\frac{n-1}{2}} \exp \left(-\frac{S_c(\phi)}{2 \sigma_w^2}\right)
\end{equation*}
where
\begin{equation*}
S_c(\phi) = \sum_{t=2}^T (x_t -\phi x_{t-1})^2 .
\end{equation*}
The latter is called the conditional sum of squares and <span class="math inline">\(\phi\)</span> can be estimated as a straightforward linear regression problem. Once an estimate <span class="math inline">\(\hat{\phi}\)</span> is obtained, this can be used to obtain the conditional maximum likelihood estimate of <span class="math inline">\(\sigma_w^2\)</span>
\begin{equation*}
\hat{\sigma}_w^2 = \frac{S_c(\hat{\phi})}{(n-1)} .
\end{equation*}
The estimation methods presented so far are standard ones for these kind of models. Nevertheless, if the data suffers from some form of contamination, these methods can become highly biased. For this reason, some robust estimators are available to limit this problematic if there are indeed outliers in the observed time series. A first solution is given by the estimator proposed in Kunsch (1984) who underlines that the MLE score function of an AR(p) is given by
\begin{equation*}
 \kappa(\mathbf{\theta}|x_j,...x_{j+p}) = \frac{\partial}{\partial \mathbf{\theta}} (x_{j+p} - \sum_{k=1}^p \phi_k x_{j+p-k})^2
\end{equation*}
where <span class="math inline">\(\theta\)</span> is the parameter vector containing, in the case of an AR(1) model, the two parameters <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\sigma_w^2\)</span> (i.e. <span class="math inline">\(\theta = [\phi \,\, \sigma_w^2]\)</span>). This delivers the estimating equation
\begin{equation*}
\sum_{j=1}^{n-p} \kappa (\hat{\mathbf{\theta}}|x_j,...x_{j+p}) = 0 .
\end{equation*}
The score function <span class="math inline">\(\kappa(\cdot)\)</span> is clearly not bounded, in the sense that if we arbitrarily move a value of <span class="math inline">\((x_t)\)</span> to infinity then the score function also goes to infinity thereby delivering a biased estimation procedure. To avoid that outlying observations bias the estimation excessively, a bounded score function can be used to deliver an M-estimator given by
\begin{equation*}
\sum_{j=1}^{n-p} \psi (\hat{\mathbf{\theta}}|x_j,...x_{j+p}) = 0,
\end{equation*}
where <span class="math inline">\(\psi(\cdot)\)</span> is a function of bounded variation. When conditioning on the first <span class="math inline">\(p\)</span> observations, this problem can be brought back to a linear regression problem which can be applied in a robust manner using the robust regression tools available in R such as <code>rlm(...)</code> or <code>lmrob(...)</code>. However, another available tool in R which can applied directly without conditioning also for general ARMA models is the <code>gmwm(...)</code> function which, by specifying the option <code>robust = TRUE</code>. This function makes use of a quantity called the wavelet variance (denoted as <span class="math inline">\(\nu\)</span>) which is estimated robustly and then used to retrieve the parameters <span class="math inline">\(\theta\)</span> of the time series model. The robust estimate is obtained by solving the following minimization problem
\begin{equation*}
\hat{\theta} = \underset{\theta \in \Theta}{\argmin} (\hat{\nu} - \nu{\theta})^T\Omega(\hat{\nu} - \nu{\theta}),
\end{equation*}
<p>where <span class="math inline">\(\hat{\nu}\)</span> is the robustly estimated wavelet variance, <span class="math inline">\(\nu{\theta}\)</span> is the theoretical wavelet variance and <span class="math inline">\(\Omega\)</span> is positive definite weighting matrix. Below we show some simulation studies where we present the results of the above estimation procedures in absence and in presence of contamination in the data.</p>
<p>INSERT SIMULATIONS</p>
<p>For all the above methods, it would be necessary to understand how “precise” their estimates are. To do so we would need to obtain confidence intervals for these estimates and this can be done mainly in two manners:</p>
<ul>
<li>using the asymptotic distribution of the parameter estimates;</li>
<li>using parametric bootrstrap.</li>
</ul>
<p>The first approach consists in using the asymptotic distribution of the estimators presented earlier to deliver approximations of the confidence intervals which get better as the length of the observed time series increases. Hence, for example, if we wanted to find a 95% confidence interval for the parameter <span class="math inline">\(\phi\)</span>, we would use the quantiles of the normal distribution (given that all methods presented earlier present this asymptotic distribution). However, this approach can present some drawbacks, one of which is there behaviour when the parameters are close to the boundaries of the parameter space. Let us take the example of an AR(1) to illustrate this problematic and suppose that <span class="math inline">\(\phi = 0.99 &lt; 1\)</span> and <span class="math inline">\(\sigma_W^2\)</span>. It can be seen that the parameter <span class="math inline">\(\phi\)</span> respects the condition for stationarity but is very close to its boundary. The code below computes the confidence interval for <span class="math inline">\(\phi\)</span> using the asymptotic normal distribution.</p>
<pre><code>## [1] 0.9840441 1.0068569</code></pre>
<p>It can be seen how the confidence interval contains values that make the AR(1) non-stationary (i.e. values of <span class="math inline">\(\phi\)</span> larger than 1). For this purpose, the approach based on parametric bootstrap provides a viable solution. Indeed, parametric bootstrap takes the estimated parameter values and uses them in order to simulate from an AR(1) based on these parameter values. For each simulation the parameters are estimated again and saved. Finally, the empirical quantiles of the saved estimated parameter values provide a confidence interval which does not suffer from boundary problems. The code below gives an example of how this confidence interval is built based on the same estimation procedure but using parametric bootstrap (using 500 bootstrap replicates).</p>
<pre><code>##      2.5%     97.5% 
## 0.8764531 0.9989982</code></pre>
<p>In this case, it can be observed that the confidence interval lies entirely within the boundaries of the parameter space.</p>

</div>
</div>
</div>



<!-- </div> -->
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-shumway2010time">
<p>Shumway, R.H., and D.S. Stoffer. 2010. <em>Time Series Analysis and Its Applications: With R Examples</em>. Springer Texts in Statistics. Springer New York. <a href="https://books.google.com/books?id=NIhXa6UeF2cC" class="uri">https://books.google.com/books?id=NIhXa6UeF2cC</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-operators-and-processes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendixa.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/SMAC-Group/TTS/edit/master/03-arma.Rmd",
"text": "Edit"
},
"download": ["tts.pdf", "tts.epub", "tts.mobi"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
